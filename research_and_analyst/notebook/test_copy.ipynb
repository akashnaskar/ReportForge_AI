{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5453311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0818a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to path: /Users/apple/Documents/Dev_Stuff/autoresearch/self_learning\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Get project root â€” one level up from 'research_and_analyst'\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added to path:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efe453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_and_analyst.utils.model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e55ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-11-14T15:52:58.740124Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-11-14T15:52:58.740804Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-11-14T15:52:58.741359Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-11-14T15:52:58.741988Z\", \"level\": \"info\", \"event\": \"Loaded ASTRA_DB_API_ENDPOINT from individual env var\"}\n",
      "{\"timestamp\": \"2025-11-14T15:52:58.742462Z\", \"level\": \"info\", \"event\": \"Loaded ASTRA_DB_APPLICATION_TOKEN from individual env var\"}\n",
      "{\"timestamp\": \"2025-11-14T15:52:58.742842Z\", \"level\": \"info\", \"event\": \"Loaded ASTRA_DB_KEYSPACE from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_o4...\", \"GOOGLE_API_KEY\": \"AIzaSy...\", \"ASTRA_DB_API_ENDPOINT\": \"https:...\", \"ASTRA_DB_APPLICATION_TOKEN\": \"AstraC...\", \"ASTRA_DB_KEYSPACE\": \"defaul...\"}, \"timestamp\": \"2025-11-14T15:52:58.743333Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"astra_db\", \"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-11-14T15:52:58.746112Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n"
     ]
    }
   ],
   "source": [
    "model_loader = ModelLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3e9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-11-14T15:53:00.175524Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763135580.187930  453222 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "llm=model_loader.load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d585280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91769b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a9b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AIMessage,HumanMessage , SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd30ec",
   "metadata": {},
   "source": [
    "#health\n",
    "\n",
    "Analyst(\n",
    "        name=\"Dr. Neha Patel\",\n",
    "        role=\"Medical Data Scientist\",\n",
    "        affiliation=\"Stanford Medicine\",\n",
    "        description=\"Focuses on predictive models for patient outcomes.\"\n",
    "        ),\n",
    "\n",
    "Analyst(\n",
    "    name=\"Dr. Arun Verma\",\n",
    "    role=\"Ethics Researcher\",\n",
    "    affiliation=\"WHO\",\n",
    "    description=\"Explores ethical implications of AI in diagnostics.\"\n",
    "),\n",
    "Analyst(\n",
    "    name=\"Ms. Priya Sharma\",\n",
    "    role=\"Policy Analyst\",\n",
    "    affiliation=\"Ministry of Health\",\n",
    "    description=\"Investigates AI policy and compliance frameworks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0d1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyst(BaseModel):\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "    role: str = Field(description=\"Role of the analyst in the context of the topic.\")\n",
    "    affiliation: str = Field(description=\"Primary affiliation of the analyst.\")\n",
    "    description: str = Field(description=\"Description of the analyst focus, concerns, and motives.\")\n",
    "    \n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278c7ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(name='Akash Naskar', role='genai eng', affiliation='AI Research LAB', description='I am genai developer as well as mentor')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyst(\n",
    "    name=\"Akash Naskar\",\n",
    "    role=\"genai eng\",\n",
    "    affiliation=\"AI Research LAB\",\n",
    "    description=\"I am genai developer as well as mentor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = Analyst(\n",
    "    name=\"sunny savita\",\n",
    "    role=\"genai eng\",\n",
    "    affiliation=\"AI Research LAB\",\n",
    "    description=\"I am genai developer as well as mentor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9ba7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunny savita'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a147e7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genai eng'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst.role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "449cd50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Research LAB'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst.affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05f5b7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sunny savita\n",
      "Role: genai eng\n",
      "Affiliation: AI Research LAB\n",
      "Description: I am genai developer as well as mentor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analyst.persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "208c2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perspectives(BaseModel):\n",
    "       analysts: List[Analyst] = Field(description=\"Comprehensive list of analysts with their roles and affiliations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5dd3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: str #research topic\n",
    "    max_analysts: int # number of analyst\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    analysts: List[Analyst] # Analyst asking questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9ea018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'finance',\n",
       " 'max_analysts': 5,\n",
       " 'human_analyst_feedback': 'give the real info'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateAnalystsState(\n",
    "    topic = \"finance\",\n",
    "    max_analysts= 5,\n",
    "    human_analyst_feedback= \"give the real info\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b4a3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Analyst(name='Dr. Neha Patel', role='Medical Data Scientist', affiliation='Stanford Medicine', description='Focuses on predictive models for patient outcomes.'),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyst(\n",
    "        name=\"Dr. Neha Patel\",\n",
    "        role=\"Medical Data Scientist\",\n",
    "        affiliation=\"Stanford Medicine\",\n",
    "        description=\"Focuses on predictive models for patient outcomes.\"\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70ffa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_instructions=\"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic:\n",
    "{topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae9b3672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\\n\\n1. First, review the research topic:\\neducation\\n\\n2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \\n\\nplease exaplain only on AI\\n\\n3. Determine the most interesting themes based upon documents and / or feedback above.\\n\\n4. Pick the top 4 themes.\\n\\n5. Assign one analyst to each theme.', 'Generate the set of analysts.']\n"
     ]
    }
   ],
   "source": [
    "print([analyst_instructions.format(\n",
    "        topic=\"education\",\n",
    "        max_analysts=4,\n",
    "        human_analyst_feedback=\"please exaplain only on AI\"\n",
    "        \n",
    "        )] + [\"Generate the set of analysts.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a176d",
   "metadata": {},
   "source": [
    "['You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\\n\\n1. First, review the research topic:\\neducation\\n\\n2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \\n\\nplease exaplain only on AI\\n\\n3. Determine the most interesting themes based upon documents and / or feedback above.\\n\\n4. Pick the top 4 themes.\\n\\n5. Assign one analyst to each theme.', 'Generate the set of analysts.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e7d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analyst(state:GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    it is creating my analyst\n",
    "    \n",
    "    \"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\",\"\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "    system_messages = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        max_analysts=max_analysts,\n",
    "        human_analyst_feedback=human_analyst_feedback\n",
    "        \n",
    "        )\n",
    "    analysts = structured_llm.invoke([SystemMessage(content=system_messages)]+ [HumanMessage(content=\"Generate the set of analysts.\")])\n",
    "    \n",
    "    # Write the list of analysis to state\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44d0a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(name='Dr. Emily Carter', role='Public Health Researcher', affiliation='National Institutes of Health', description='Focuses on preventative healthcare and the impact of public health policies on community well-being. Concerned with health equity and access to care for underserved populations.'),\n",
       "  Analyst(name='Dr. Michael Chen', role='Healthcare Economist', affiliation='Harvard University', description='Analyzes healthcare costs, insurance models, and the economic impact of healthcare regulations. Motivated by finding sustainable and efficient solutions for healthcare financing.')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_analyst(\n",
    "    {'topic': 'health',\n",
    "    'max_analysts': 2,\n",
    "    'human_analyst_feedback': 'give the real info'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acbd701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(state):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abcfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    feedback = (state.get(\"human_analyst_feedback\") or \"\").strip().lower()\n",
    "    if feedback and feedback not in [\"\", \"none\", \"skip\", \"done\", \"continue\"]:\n",
    "        return \"create_analyst\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4aef85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def should_continue(state):\n",
    "#     \"\"\" Return the next node to execute \"\"\"\n",
    "#     human_analyst_feedback = state.get(\"human_analyst_feedback\",None)\n",
    "#     if human_analyst_feedback:\n",
    "#         return \"create_analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a7596bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e942924",
   "metadata": {},
   "source": [
    "## First Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69a8b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(GenerateAnalystsState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "914c955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1235ee950>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"create_analyst\",create_analyst)\n",
    "builder.add_node(\"human_feedback\", human_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd30a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1235ee950>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(START,\"create_analyst\")\n",
    "builder.add_edge(\"create_analyst\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\",\n",
    "                        should_continue,\n",
    "                        [\"create_analyst\",\n",
    "                        END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a60c6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73530c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(interrupt_before= [\"human_feedback\"],checkpointer= memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37a56840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6bd1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"the benefits of adopting Langgraph as an agent framework\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca5b3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_analysts = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e3850f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread =  {\"configurable\":{\"thread_id\":1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe662e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice Chen\n",
      "Affiliation: Independent AI Lab\n",
      "Role: AI Research Scientist\n",
      "Description: Focuses on the technical advantages of Langgraph, such as its ability to manage complex agent workflows and improve the efficiency of multi-agent systems. She is motivated by the potential for Langgraph to advance the state of the art in AI research.\n",
      "--------------------------------------------------\n",
      "Name: Bob Johnson\n",
      "Affiliation: Large Financial Institution\n",
      "Role: Enterprise Architect\n",
      "Description: Concerned with the practical benefits of Langgraph for enterprise applications. His focus is on how Langgraph can improve the reliability, scalability, and maintainability of AI-powered systems in a business context. He is also interested in the cost savings and ROI that Langgraph can provide.\n",
      "--------------------------------------------------\n",
      "Name: Charlie Davis\n",
      "Affiliation: Non-profit AI Ethics Organization\n",
      "Role: AI Ethics Consultant\n",
      "Description: Prioritizes the ethical implications of using Langgraph. He is interested in how Langgraph can be used to build more transparent, accountable, and fair AI systems. He is also concerned about the potential for Langgraph to be used to create biased or harmful AI applications.\n",
      "--------------------------------------------------\n",
      "Name: Diana Rodriguez\n",
      "Affiliation: AI Startup\n",
      "Role: AI Product Manager\n",
      "Description: Evaluates Langgraph from a product development perspective. She is interested in how Langgraph can accelerate the development and deployment of new AI products and features. She is also focused on the user experience and how Langgraph can make it easier for developers to build and use AI-powered applications.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"topic\":topic,\n",
    "              \"max_analysts\":max_analysts},\n",
    "             thread,\n",
    "             stream_mode= \"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    \n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7b08aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63c172ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'the benefits of adopting Langgraph as an agent framework', 'max_analysts': 4, 'analysts': [Analyst(name='Alice Chen', role='AI Research Scientist', affiliation='Independent AI Lab', description='Focuses on the technical advantages of Langgraph, such as its ability to manage complex agent workflows and improve the efficiency of multi-agent systems. She is motivated by the potential for Langgraph to advance the state of the art in AI research.'), Analyst(name='Bob Johnson', role='Enterprise Architect', affiliation='Large Financial Institution', description='Concerned with the practical benefits of Langgraph for enterprise applications. His focus is on how Langgraph can improve the reliability, scalability, and maintainability of AI-powered systems in a business context. He is also interested in the cost savings and ROI that Langgraph can provide.'), Analyst(name='Charlie Davis', role='AI Ethics Consultant', affiliation='Non-profit AI Ethics Organization', description='Prioritizes the ethical implications of using Langgraph. He is interested in how Langgraph can be used to build more transparent, accountable, and fair AI systems. He is also concerned about the potential for Langgraph to be used to create biased or harmful AI applications.'), Analyst(name='Diana Rodriguez', role='AI Product Manager', affiliation='AI Startup', description='Evaluates Langgraph from a product development perspective. She is interested in how Langgraph can accelerate the development and deployment of new AI products and features. She is also focused on the user experience and how Langgraph can make it easier for developers to build and use AI-powered applications.')]}, next=('human_feedback',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0abe9b-d35d-63fc-8001-2abc356ff911'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-10-18T06:14:43.863619+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0abe9b-b57f-6240-8000-c4a87c7e907f'}}, tasks=(PregelTask(id='f9e98e3c-bfb1-3bbe-f7d3-d9bc9792ee16', name='human_feedback', path=('__pregel_pull', 'human_feedback'), error=None, interrupts=(), state=None, result=None),), interrupts=())"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d2a5eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'the benefits of adopting Langgraph as an agent framework',\n",
       " 'max_analysts': 4,\n",
       " 'analysts': [Analyst(name='Alice Chen', role='AI Research Scientist', affiliation='Independent AI Lab', description='Focuses on the technical advantages of Langgraph, such as its ability to manage complex agent workflows and improve the efficiency of multi-agent systems. She is motivated by the potential for Langgraph to advance the state of the art in AI research.'),\n",
       "  Analyst(name='Bob Johnson', role='Enterprise Architect', affiliation='Large Financial Institution', description='Concerned with the practical benefits of Langgraph for enterprise applications. His focus is on how Langgraph can improve the reliability, scalability, and maintainability of AI-powered systems in a business context. He is also interested in the cost savings and ROI that Langgraph can provide.'),\n",
       "  Analyst(name='Charlie Davis', role='AI Ethics Consultant', affiliation='Non-profit AI Ethics Organization', description='Prioritizes the ethical implications of using Langgraph. He is interested in how Langgraph can be used to build more transparent, accountable, and fair AI systems. He is also concerned about the potential for Langgraph to be used to create biased or harmful AI applications.'),\n",
       "  Analyst(name='Diana Rodriguez', role='AI Product Manager', affiliation='AI Startup', description='Evaluates Langgraph from a product development perspective. She is interested in how Langgraph can accelerate the development and deployment of new AI products and features. She is also focused on the user experience and how Langgraph can make it easier for developers to build and use AI-powered applications.')]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e772e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "140be2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "231450e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('1', defaultdict(<class 'dict'>, {'': {'1f0abe9b-b57b-6af0-bfff-662a2e9ef077': (('msgpack', b'\\x86\\xa1v\\x04\\xa2ts\\xd9 2025-10-18T06:14:40.730385+00:00\\xa2id\\xd9$1f0abe9b-b57b-6af0-bfff-662a2e9ef077\\xb0channel_versions\\x81\\xa9__start__\\xd9400000000000000000000000000000001.0.19358629096087598\\xadversions_seen\\x81\\xa9__input__\\x80\\xb0updated_channels\\x91\\xa9__start__'), ('msgpack', b'\\x83\\xa6source\\xa5input\\xa4step\\xff\\xa7parents\\x80'), None), '1f0abe9b-b57f-6240-8000-c4a87c7e907f': (('msgpack', b'\\x86\\xa1v\\x04\\xa2ts\\xd9 2025-10-18T06:14:40.731799+00:00\\xa2id\\xd9$1f0abe9b-b57f-6240-8000-c4a87c7e907f\\xb0channel_versions\\x84\\xa9__start__\\xd9400000000000000000000000000000002.0.23794191427870615\\xa5topic\\xd9400000000000000000000000000000002.0.23794191427870615\\xacmax_analysts\\xd9400000000000000000000000000000002.0.23794191427870615\\xb8branch:to:create_analyst\\xd9400000000000000000000000000000002.0.23794191427870615\\xadversions_seen\\x82\\xa9__input__\\x80\\xa9__start__\\x81\\xa9__start__\\xd9400000000000000000000000000000001.0.19358629096087598\\xb0updated_channels\\x93\\xb8branch:to:create_analyst\\xacmax_analysts\\xa5topic'), ('msgpack', b'\\x83\\xa6source\\xa4loop\\xa4step\\x00\\xa7parents\\x80'), '1f0abe9b-b57b-6af0-bfff-662a2e9ef077'), '1f0abe9b-d35d-63fc-8001-2abc356ff911': (('msgpack', b'\\x86\\xa1v\\x04\\xa2ts\\xd9 2025-10-18T06:14:43.863619+00:00\\xa2id\\xd9$1f0abe9b-d35d-63fc-8001-2abc356ff911\\xb0channel_versions\\x86\\xa9__start__\\xd9400000000000000000000000000000002.0.23794191427870615\\xa5topic\\xd9400000000000000000000000000000002.0.23794191427870615\\xacmax_analysts\\xd9400000000000000000000000000000002.0.23794191427870615\\xb8branch:to:create_analyst\\xd9300000000000000000000000000000003.0.6714411191940287\\xa8analysts\\xd9300000000000000000000000000000003.0.6714411191940287\\xb8branch:to:human_feedback\\xd9300000000000000000000000000000003.0.6714411191940287\\xadversions_seen\\x83\\xa9__input__\\x80\\xa9__start__\\x81\\xa9__start__\\xd9400000000000000000000000000000001.0.19358629096087598\\xaecreate_analyst\\x81\\xb8branch:to:create_analyst\\xd9400000000000000000000000000000002.0.23794191427870615\\xb0updated_channels\\x92\\xa8analysts\\xb8branch:to:human_feedback'), ('msgpack', b'\\x83\\xa6source\\xa4loop\\xa4step\\x01\\xa7parents\\x80'), '1f0abe9b-b57f-6240-8000-c4a87c7e907f')}}))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.storage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4747e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import msgpack\n",
    "\n",
    "# for thread_id, ns_dict in memory.storage.items():\n",
    "#     print(f\"\\n Thread ID: {thread_id}\")\n",
    "    \n",
    "#     # ns_dict = defaultdict(dict, {'': {...}})\n",
    "#     for ns, ckpts in ns_dict.items():\n",
    "#         print(f\"  Namespace: '{ns}'\")\n",
    "        \n",
    "#         # ckpts = dict of {checkpoint_id: (packed_values, packed_metadata, parent_id)}\n",
    "#         for ckpt_id, (packed_values, packed_metadata, parent_id) in ckpts.items():\n",
    "#             print(f\"    Checkpoint ID: {ckpt_id}\")\n",
    "            \n",
    "#             # Decode msgpack binary\n",
    "#             values = msgpack.unpackb(packed_values[1], raw=False)\n",
    "#             meta = msgpack.unpackb(packed_metadata[1], raw=False)\n",
    "            \n",
    "#             print(f\"    Values keys: {list(values.keys())}\")\n",
    "#             print(f\"    Parent ID: {parent_id}\")\n",
    "#             print(f\"    Metadata: {meta}\")\n",
    "#             print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a867143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0abe9b-d35d-63fc-8001-2abc356ff911'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb7d8558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0abe9d-8169-6c1a-8002-d561f4040a8c'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(thread,\n",
    "                   {\"human_analyst_feedback\":\"add something from the startup perspective and focus on the latest enterprise application\"},as_node=\"human_feedback\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92f6e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice Chen\n",
      "Affiliation: Independent AI Lab\n",
      "Role: AI Research Scientist\n",
      "Description: Focuses on the technical advantages of Langgraph, such as its ability to manage complex agent workflows and improve the efficiency of multi-agent systems. She is motivated by the potential for Langgraph to advance the state of the art in AI research.\n",
      "--------------------------------------------------\n",
      "Name: Bob Johnson\n",
      "Affiliation: Large Financial Institution\n",
      "Role: Enterprise Architect\n",
      "Description: Concerned with the practical benefits of Langgraph for enterprise applications. His focus is on how Langgraph can improve the reliability, scalability, and maintainability of AI-powered systems in a business context. He is also interested in the cost savings and ROI that Langgraph can provide.\n",
      "--------------------------------------------------\n",
      "Name: Charlie Davis\n",
      "Affiliation: Non-profit AI Ethics Organization\n",
      "Role: AI Ethics Consultant\n",
      "Description: Prioritizes the ethical implications of using Langgraph. He is interested in how Langgraph can be used to build more transparent, accountable, and fair AI systems. He is also concerned about the potential for Langgraph to be used to create biased or harmful AI applications.\n",
      "--------------------------------------------------\n",
      "Name: Diana Rodriguez\n",
      "Affiliation: AI Startup\n",
      "Role: AI Product Manager\n",
      "Description: Evaluates Langgraph from a product development perspective. She is interested in how Langgraph can accelerate the development and deployment of new AI products and features. She is also focused on the user experience and how Langgraph can make it easier for developers to build and use AI-powered applications.\n",
      "--------------------------------------------------\n",
      "Name: Ava Chen\n",
      "Affiliation: InnovateAI\n",
      "Role: Startup CTO\n",
      "Description: Focuses on Langgraph's rapid prototyping and deployment capabilities for AI-powered startups, emphasizing its potential to accelerate development cycles and reduce time to market. Concerned with ease of integration, scalability, and cost-effectiveness for early-stage ventures.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Ben Carter\n",
      "Affiliation: GlobalTech Solutions\n",
      "Role: Enterprise AI Architect\n",
      "Description: Evaluates Langgraph's suitability for complex enterprise applications, focusing on its ability to manage multi-agent workflows, ensure reliability, and integrate with existing enterprise systems. Prioritizes security, compliance, and governance in large-scale deployments.\n",
      "--------------------------------------------------\n",
      "Name: Priya Sharma\n",
      "Affiliation: AI Research Institute\n",
      "Role: AI Research Scientist\n",
      "Description: Investigates Langgraph's research potential, particularly its modularity and flexibility for experimenting with novel agent architectures and collaboration strategies. Interested in pushing the boundaries of AI capabilities and exploring new frontiers in agent-based problem-solving.\n",
      "--------------------------------------------------\n",
      "Name: David Lee\n",
      "Affiliation: ProductAI Inc.\n",
      "Role: AI Product Manager\n",
      "Description: Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"topic\":topic,\n",
    "              \"max_analysts\":max_analysts},\n",
    "             thread,\n",
    "             stream_mode= \"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    \n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "655163ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffc316df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'the benefits of adopting Langgraph as an agent framework', 'max_analysts': 4, 'human_analyst_feedback': 'add something from the startup perspective and focus on the latest enterprise application', 'analysts': [Analyst(name='Ava Chen', role='Startup CTO', affiliation='InnovateAI', description=\"Focuses on Langgraph's rapid prototyping and deployment capabilities for AI-powered startups, emphasizing its potential to accelerate development cycles and reduce time to market. Concerned with ease of integration, scalability, and cost-effectiveness for early-stage ventures.\"), Analyst(name='Dr. Ben Carter', role='Enterprise AI Architect', affiliation='GlobalTech Solutions', description=\"Evaluates Langgraph's suitability for complex enterprise applications, focusing on its ability to manage multi-agent workflows, ensure reliability, and integrate with existing enterprise systems. Prioritizes security, compliance, and governance in large-scale deployments.\"), Analyst(name='Priya Sharma', role='AI Research Scientist', affiliation='AI Research Institute', description=\"Investigates Langgraph's research potential, particularly its modularity and flexibility for experimenting with novel agent architectures and collaboration strategies. Interested in pushing the boundaries of AI capabilities and exploring new frontiers in agent-based problem-solving.\"), Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\")]}, next=('human_feedback',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0abe9d-d416-68dc-8005-da310fbe43e0'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-10-18T06:15:37.626604+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0abe9d-b128-6c1c-8004-7671837bc1b7'}}, tasks=(PregelTask(id='63b66b3c-61b5-5f42-7c4b-495350fb91dc', name='human_feedback', path=('__pregel_pull', 'human_feedback'), error=None, interrupts=(), state=None, result=None),), interrupts=())"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "055b0545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f86148ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'the benefits of adopting Langgraph as an agent framework',\n",
       " 'max_analysts': 4,\n",
       " 'human_analyst_feedback': 'add something from the startup perspective and focus on the latest enterprise application',\n",
       " 'analysts': [Analyst(name='Ava Chen', role='Startup CTO', affiliation='InnovateAI', description=\"Focuses on Langgraph's rapid prototyping and deployment capabilities for AI-powered startups, emphasizing its potential to accelerate development cycles and reduce time to market. Concerned with ease of integration, scalability, and cost-effectiveness for early-stage ventures.\"),\n",
       "  Analyst(name='Dr. Ben Carter', role='Enterprise AI Architect', affiliation='GlobalTech Solutions', description=\"Evaluates Langgraph's suitability for complex enterprise applications, focusing on its ability to manage multi-agent workflows, ensure reliability, and integrate with existing enterprise systems. Prioritizes security, compliance, and governance in large-scale deployments.\"),\n",
       "  Analyst(name='Priya Sharma', role='AI Research Scientist', affiliation='AI Research Institute', description=\"Investigates Langgraph's research potential, particularly its modularity and flexibility for experimenting with novel agent architectures and collaboration strategies. Interested in pushing the boundaries of AI capabilities and exploring new frontiers in agent-based problem-solving.\"),\n",
       "  Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\")]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0540c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import msgpack\n",
    "\n",
    "# def get_all_checkpoints(memory, thread_id=\"1\"):\n",
    "#     \"\"\"Return all checkpoints in chronological order for a thread.\"\"\"\n",
    "#     checkpoints = []\n",
    "#     ns_dict = memory.storage.get(thread_id, {})\n",
    "#     if \"\" not in ns_dict:\n",
    "#         return []\n",
    "\n",
    "#     for ckpt_id, (packed_values, packed_meta, parent_id) in ns_dict[\"\"].items():\n",
    "#         values = msgpack.unpackb(packed_values[1], raw=False)\n",
    "#         meta = msgpack.unpackb(packed_meta[1], raw=False)\n",
    "#         checkpoints.append({\n",
    "#             \"id\": ckpt_id,\n",
    "#             \"parent\": parent_id,\n",
    "#             \"topic\": values.get(\"topic\"),\n",
    "#             \"feedback\": values.get(\"human_analyst_feedback\"),\n",
    "#             \"analyst_count\": len(values.get(\"analysts\", [])),\n",
    "#             \"analysts\": [a.model_dump() for a in values.get(\"analysts\", [])],\n",
    "#             \"step\": meta.get(\"step\"),\n",
    "#             \"created_at\": values.get(\"ts\", None)\n",
    "#         })\n",
    "#     return checkpoints\n",
    "\n",
    "# # Fetch all\n",
    "# history = get_all_checkpoints(memory)\n",
    "\n",
    "# # Sort by step (to get chronological order)\n",
    "# history = sorted(history, key=lambda x: (x[\"step\"] or 0))\n",
    "\n",
    "# # Display neatly\n",
    "# for h in history:\n",
    "#     print(f\"\\nSTEP {h['step']} | CHECKPOINT {h['id']}\")\n",
    "#     print(f\"Parent: {h['parent']}\")\n",
    "#     print(f\"Topic: {h['topic']}\")\n",
    "#     print(f\"Feedback: {h['feedback']}\")\n",
    "#     print(f\"Analysts generated: {h['analyst_count']}\")\n",
    "#     print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a17375c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are satisfied, then we simply supply no feedback\n",
    "further_feedack = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cac0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get the latest state (you're paused at 'human_feedback')\n",
    "state = graph.get_state(thread)\n",
    "\n",
    "# 2) Use the exact config from that state (it already has thread_id, checkpoint_ns, checkpoint_id)\n",
    "cfg = state.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82420b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0abe9e-32a8-6756-8006-a78e9b3f5c1a'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Update feedback at the 'human_feedback' node\n",
    "#    Tip: if your TypedDict says `human_analyst_feedback: str`, prefer \"\" (empty string) over None\n",
    "graph.update_state(cfg, {\"human_analyst_feedback\": \"\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aceb37c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# 4) Verify it moved to END\n",
    "final_state = graph.get_state(thread)\n",
    "print(final_state.next)  # should be (END,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "763cb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysts = final_state.values.get('analysts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e26234f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Analyst(name='Ava Chen', role='Startup CTO', affiliation='InnovateAI', description=\"Focuses on Langgraph's rapid prototyping and deployment capabilities for AI-powered startups, emphasizing its potential to accelerate development cycles and reduce time to market. Concerned with ease of integration, scalability, and cost-effectiveness for early-stage ventures.\"),\n",
       " Analyst(name='Dr. Ben Carter', role='Enterprise AI Architect', affiliation='GlobalTech Solutions', description=\"Evaluates Langgraph's suitability for complex enterprise applications, focusing on its ability to manage multi-agent workflows, ensure reliability, and integrate with existing enterprise systems. Prioritizes security, compliance, and governance in large-scale deployments.\"),\n",
       " Analyst(name='Priya Sharma', role='AI Research Scientist', affiliation='AI Research Institute', description=\"Investigates Langgraph's research potential, particularly its modularity and flexibility for experimenting with novel agent architectures and collaboration strategies. Interested in pushing the boundaries of AI capabilities and exploring new frontiers in agent-based problem-solving.\"),\n",
       " Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "753e9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ava Chen\n",
      "Affiliation: InnovateAI\n",
      "Role: Startup CTO\n",
      "Description: Focuses on Langgraph's rapid prototyping and deployment capabilities for AI-powered startups, emphasizing its potential to accelerate development cycles and reduce time to market. Concerned with ease of integration, scalability, and cost-effectiveness for early-stage ventures.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Ben Carter\n",
      "Affiliation: GlobalTech Solutions\n",
      "Role: Enterprise AI Architect\n",
      "Description: Evaluates Langgraph's suitability for complex enterprise applications, focusing on its ability to manage multi-agent workflows, ensure reliability, and integrate with existing enterprise systems. Prioritizes security, compliance, and governance in large-scale deployments.\n",
      "--------------------------------------------------\n",
      "Name: Priya Sharma\n",
      "Affiliation: AI Research Institute\n",
      "Role: AI Research Scientist\n",
      "Description: Investigates Langgraph's research potential, particularly its modularity and flexibility for experimenting with novel agent architectures and collaboration strategies. Interested in pushing the boundaries of AI capabilities and exploring new frontiers in agent-based problem-solving.\n",
      "--------------------------------------------------\n",
      "Name: David Lee\n",
      "Affiliation: ProductAI Inc.\n",
      "Role: AI Product Manager\n",
      "Description: Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c15bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The benefits of adopting LangGraph as an agent framework\"\n",
    "query = \"The benefits of adopting LangGraph as an agent framework\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c36ad6b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m docs = \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLangGraph\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[32m0\u001b[39m].page_content[:\u001b[32m500\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/document_loaders/wikipedia.py:59\u001b[39m, in \u001b[36mWikipediaLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoads the query result from Wikipedia into a list of Documents.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        Wikipedia pages.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m client = WikipediaAPIWrapper(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     54\u001b[39m     lang=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m     55\u001b[39m     top_k_results=\u001b[38;5;28mself\u001b[39m.load_max_docs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     56\u001b[39m     load_all_available_meta=\u001b[38;5;28mself\u001b[39m.load_all_available_meta,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m     doc_content_chars_max=\u001b[38;5;28mself\u001b[39m.doc_content_chars_max,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:111\u001b[39m, in \u001b[36mWikipediaAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:125\u001b[39m, in \u001b[36mWikipediaAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    121\u001b[39m page_titles = \u001b[38;5;28mself\u001b[39m.wiki_client.search(\n\u001b[32m    122\u001b[39m     query[:WIKIPEDIA_MAX_QUERY_LENGTH], results=\u001b[38;5;28mself\u001b[39m.top_k_results\n\u001b[32m    123\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m.top_k_results]:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wiki_page := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_title\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m doc := \u001b[38;5;28mself\u001b[39m._page_to_document(page_title, wiki_page):\n\u001b[32m    127\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:96\u001b[39m, in \u001b[36mWikipediaAPIWrapper._fetch_page\u001b[39m\u001b[34m(self, page)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_page\u001b[39m(\u001b[38;5;28mself\u001b[39m, page: \u001b[38;5;28mstr\u001b[39m) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwiki_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_suggest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m     98\u001b[39m         \u001b[38;5;28mself\u001b[39m.wiki_client.exceptions.PageError,\n\u001b[32m     99\u001b[39m         \u001b[38;5;28mself\u001b[39m.wiki_client.exceptions.DisambiguationError,\n\u001b[32m    100\u001b[39m     ):\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:276\u001b[39m, in \u001b[36mpage\u001b[39m\u001b[34m(title, pageid, auto_suggest, redirect, preload)\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m    274\u001b[39m       \u001b[38;5;66;03m# if there is no suggestion or search results, the page doesn't exist\u001b[39;00m\n\u001b[32m    275\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m PageError(title)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWikipediaPage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pageid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m WikipediaPage(pageid=pageid, preload=preload)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:299\u001b[39m, in \u001b[36mWikipediaPage.__init__\u001b[39m\u001b[34m(self, title, pageid, redirect, preload, original_title)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    297\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEither a title or a pageid must be specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preload:\n\u001b[32m    302\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreferences\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlinks\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msections\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:336\u001b[39m, in \u001b[36mWikipediaPage.__load\u001b[39m\u001b[34m(self, redirect, preload)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m   query_params[\u001b[33m'\u001b[39m\u001b[33mpageids\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.pageid\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m request = \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m query = request[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    339\u001b[39m pageid = \u001b[38;5;28mlist\u001b[39m(query[\u001b[33m'\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m'\u001b[39m].keys())[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[39m, in \u001b[36m_wiki_request\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    734\u001b[39m   wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n\u001b[32m    735\u001b[39m   time.sleep(\u001b[38;5;28mint\u001b[39m(wait_time.total_seconds()))\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[32m    740\u001b[39m   RATE_LIMIT_LAST_CALL = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"LangGraph\").load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd8ad1",
   "metadata": {},
   "source": [
    "#### Using DuckDuckGo for web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e76bcee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=The%20benefits%20of%20adopting%20LangGraph%20as%20an%20agent%20framework): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "response: https://search.yahoo.com/search;_ylt=4-v-VGdxub8x_n0ex2NPLKYd;_ylu=xjEqWKdTA7sn-o57LTDQcYc5N4UeRYiHTUACD8z7iHBQW6E?p=The+benefits+of+adopting+LangGraph+as+an+agent+framework&btf=y 200\n",
      "response: https://search.brave.com/search?q=The+benefits+of+adopting+LangGraph+as+an+agent+framework&source=web&tf=py 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sep 4, 2025 Â· By Nuno Campos Summary: We launched LangGraph as a low level agent framework nearly two years ago, and have already seen companies like LinkedIn, Uber, and Klarna use it to build production ready agents . LangGraph builds upon feedback from the super popular LangChain framework , and rethinks how agent frameworks should work for production. We aimed to find the right abstraction for AI agents ... June 30, 2025 - Explore how LangGraph , the graph-based agent framework from LangChain, empowers developers and organizations to orchestrate complex AI workflows, integrate with any LLM, and build practical, scalable AI solutions. Mar 11, 2024 Â· LangGraph â€™ s advanced workflow control, the ability to ensure quality in function calling, the customization of message types and parameters, and convenient state and LLM token management are ... Jun 17, 2025 Â· Explore how LangGraph , the graph-based agent framework from LangChain, empowers developers and organizations to orchestrate complex AI workflows, integrate with any LLM, and build practical, scalable AI solutions. Developed by the creators of LangChain, LangGraph redefines how developers conceptualize and implement AI workflows, providing a modular and visually intuitive approach to agent architecture that scales from straightforward chatbots to advanced multi- agent cognitive systems. Sep 3, 2025 Â· What are the main advantages of using the LangGraph AI Framework for multi- agent workflows over traditional linear approaches? The LangGraph AI Framework stands out in multi- agent workflows by introducing dynamic, graph-based orchestration. Feb 19, 2025 Â· LangGraph is an innovative framework that leverages directed graphs to model AI workflows. Unlike traditional sequential or decision-tree-based logic, LangGraph allows you to define nodes (representing tasks or actions) and edges (representing the flow of information) for more flexible and scalable AI applications. Jul 14, 2025 Â· When you're building high-performing AI agents for real-world use, LangGraph offers the flexibility, reliability, and control needed to meet enterprise demands, whether you're focused on automation, compliance, or enhanced customer experiences. Here are the top features that make LangGraph stand out: 1. Graph-Based Workflow Design. Developed by the creators of LangChain, LangGraph redefines how developers conceptualize and implement AI workflows, providing a modular and visually intuitive approach to agent architecture that scales from straightforward chatbots to advanced multi- agent cognitive systems. Sep 3, 2025 Â· What are the main advantages of using the LangGraph AI Framework for multi- agent workflows over traditional linear approaches? The LangGraph AI Framework stands out in multi- agent workflows by introducing dynamic, graph-based orchestration.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "search= DuckDuckGoSearchRun()\n",
    "search.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31e069",
   "metadata": {},
   "source": [
    "\"Sep 4, 2025 Â· By Nuno Campos Summary: We launched LangGraph as a low level agent framework nearly two years ago, and have already seen companies like LinkedIn, Uber, and Klarna use it to build production ready agents . LangGraph builds upon feedback from the super popular LangChain framework , and rethinks how agent frameworks should work for production. We aimed to find the right abstraction for AI agents ... Jun 17, 2025 Â· Explore how LangGraph , the graph-based agent framework from LangChain, empowers developers and organizations to orchestrate complex AI workflows, integrate with any LLM, and build practical, scalable AI solutions. LangGraph is a Python-based framework designed to manage multi- agent workflows using graph architectures. Unlike linear processes, LangGraph organizes actions as nodes in a directed graph, enabling tasks like conditional decision-making, parallel execution, and persistent state management. Mar 11, 2024 Â· LangGraph â€™ s advanced workflow control, the ability to ensure quality in function calling, the customization of message types and parameters, and convenient state and LLM token management are ... Jun 17, 2025 Â· Explore how LangGraph , the graph-based agent framework from LangChain, empowers developers and organizations to orchestrate complex AI workflows, integrate with any LLM, and build practical, scalable AI solutions. Developed by the creators of LangChain, LangGraph redefines how developers conceptualize and implement AI workflows, providing a modular and visually intuitive approach to agent architecture that scales from straightforward chatbots to advanced multi- agent cognitive systems. Sep 3, 2025 Â· What are the main advantages of using the LangGraph AI Framework for multi- agent workflows over traditional linear approaches? The LangGraph AI Framework stands out in multi- agent workflows by introducing dynamic, graph-based orchestration. Feb 19, 2025 Â· LangGraph is an innovative framework that leverages directed graphs to model AI workflows. Unlike traditional sequential or decision-tree-based logic, LangGraph allows you to define nodes (representing tasks or actions) and edges (representing the flow of information) for more flexible and scalable AI applications. Jul 14, 2025 Â· When you're building high-performing AI agents for real-world use, LangGraph offers the flexibility, reliability, and control needed to meet enterprise demands, whether you're focused on automation, compliance, or enhanced customer experiences. Here are the top features that make LangGraph stand out: 1. Graph-Based Workflow Design. Developed by the creators of LangChain, LangGraph redefines how developers conceptualize and implement AI workflows, providing a modular and visually intuitive approach to agent architecture that scales from straightforward chatbots to advanced multi- agent cognitive systems.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9f6ef",
   "metadata": {},
   "source": [
    "Either you can use Google Serper API or use duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8520f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WikipediaLoader\n",
    "# docs = WikipediaLoader(query=\"The benefits of adopting AWS Cloud\").load()\n",
    "# print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db3f5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WikipediaLoader\n",
    "# docs = WikipediaLoader(query=\"The benefits of adopting AWS Cloud\").load()\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08292d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m docs = \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAWS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[32m0\u001b[39m].page_content[:\u001b[32m500\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/document_loaders/wikipedia.py:59\u001b[39m, in \u001b[36mWikipediaLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoads the query result from Wikipedia into a list of Documents.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        Wikipedia pages.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m client = WikipediaAPIWrapper(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     54\u001b[39m     lang=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m     55\u001b[39m     top_k_results=\u001b[38;5;28mself\u001b[39m.load_max_docs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     56\u001b[39m     load_all_available_meta=\u001b[38;5;28mself\u001b[39m.load_all_available_meta,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m     doc_content_chars_max=\u001b[38;5;28mself\u001b[39m.doc_content_chars_max,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:111\u001b[39m, in \u001b[36mWikipediaAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:126\u001b[39m, in \u001b[36mWikipediaAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m.top_k_results]:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wiki_page := \u001b[38;5;28mself\u001b[39m._fetch_page(page_title):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m doc := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_page_to_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwiki_page\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    127\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:86\u001b[39m, in \u001b[36mWikipediaAPIWrapper._page_to_document\u001b[39m\u001b[34m(self, page_title, wiki_page)\u001b[39m\n\u001b[32m     66\u001b[39m main_meta = {\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: page_title,\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.summary,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.url,\n\u001b[32m     70\u001b[39m }\n\u001b[32m     71\u001b[39m add_meta = (\n\u001b[32m     72\u001b[39m     {\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcategories\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.categories,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m     84\u001b[39m )\n\u001b[32m     85\u001b[39m doc = Document(\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     page_content=\u001b[43mwiki_page\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m[: \u001b[38;5;28mself\u001b[39m.doc_content_chars_max],\n\u001b[32m     87\u001b[39m     metadata={\n\u001b[32m     88\u001b[39m         **main_meta,\n\u001b[32m     89\u001b[39m         **add_meta,\n\u001b[32m     90\u001b[39m     },\n\u001b[32m     91\u001b[39m )\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:475\u001b[39m, in \u001b[36mWikipediaPage.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    474\u001b[39m    query_params[\u001b[33m'\u001b[39m\u001b[33mpageids\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.pageid\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m request = \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28mself\u001b[39m._content     = request[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m'\u001b[39m][\u001b[38;5;28mself\u001b[39m.pageid][\u001b[33m'\u001b[39m\u001b[33mextract\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    477\u001b[39m \u001b[38;5;28mself\u001b[39m._revision_id = request[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m'\u001b[39m][\u001b[38;5;28mself\u001b[39m.pageid][\u001b[33m'\u001b[39m\u001b[33mrevisions\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mrevid\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[39m, in \u001b[36m_wiki_request\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    734\u001b[39m   wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n\u001b[32m    735\u001b[39m   time.sleep(\u001b[38;5;28mint\u001b[39m(wait_time.total_seconds()))\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[32m    740\u001b[39m   RATE_LIMIT_LAST_CALL = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"AWS\").load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1ea76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15ad184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Semantic Web\n",
      "Summary: The Semantic Web, sometimes known as Web 3.0, is an extension of the World Wide Web through standards set by the World Wide Web Consortium (W3C). The goal of the Semantic Web is to make Internet data machine-readable.\n",
      "To enable the encoding of semantics with the data, technologies such as Resource Description Framework (RDF) and Web Ontology Language (OWL) are used. These technologies are used to formally represent metadata. For example, ontology can describe concepts, relationships between entities, and categories of things. These embedded semantics offer significant advantages such as reasoning over data and operating with heterogeneous data sources.\n",
      "These standards promote common data formats and exchange protocols on the Web, fundamentally the RDF. According to the W3C, \"The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries.\" The Semantic Web is therefore regarded as an integrator across different content and information applications and systems.\n",
      "\n",
      "\n",
      "\n",
      "Page: React (software)\n",
      "Summary: React (also known as React.js or ReactJS) is a free and open-source front-end JavaScript library that aims to make building user interfaces based on components more \"seamless\". It is maintained by Meta (formerly Facebook) and a community of individual developers and companies. According to the Stack Overflow Developer Survey, React is one of the most commonly used web technologies.\n",
      "React can be used to develop single-page, mobile, or server-rendered applications with frameworks like Next.js and React Router. Because React is only concerned with the user interface and rendering components to the DOM, React applications often rely on libraries for routing and other client-side functionality. A key advantage of React is that it only re-renders those parts of the page that have changed, avoiding unnecessary re-rendering of unchanged DOM elements. React is used by an estimated 6% of all websites.\n",
      "\n",
      "Page: Deep learning\n",
      "Summary: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\n",
      "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
      "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki = WikipediaAPIWrapper(doc_content_chars_max=4000)\n",
    "docs = wiki.run(\"The benefits of adopting LangGraph as an agentic framework\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bea3e3",
   "metadata": {},
   "source": [
    "## Second Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d7c9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38cecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef34c867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/rd80r4td7l7600vkrbdgpc6c0000gn/T/ipykernel_1473/1029971610.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(tavily_api_key=tavily_api_key)\n"
     ]
    }
   ],
   "source": [
    "tavily_search = TavilySearchResults(tavily_api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c1364b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - GeeksforGeeks',\n",
       "  'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
       "  'content': '# What is LangGraph?\\n\\nLast Updated : \\n10 Oct, 2025\\n\\nSuggest changes\\n\\n1 Likes\\n\\nLangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions.',\n",
       "  'score': 0.9345448},\n",
       " {'title': 'Learn LangGraph basics - Overview',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/',\n",
       "  'content': 'Trusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.',\n",
       "  'score': 0.9310187},\n",
       " {'title': 'LangGraph - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/',\n",
       "  'content': 'Trusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.',\n",
       "  'score': 0.9291904},\n",
       " {'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'content': 'To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of whatâ€™s possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future. [...] LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) â€“ based AI agents.\\n It views agent Objective Points about LangGraph and workflows as cyclic graph topologies, allowing for more variable and nuanced agent behaviors than linear execution models. [...] Frameworks such as LangGraph are becoming increasingly important as AI develops. LangGraph is making the next generation of AI applications possible by offering a versatile and strong framework for developing and overseeing AI agents.',\n",
       "  'score': 0.92005306},\n",
       " {'title': 'What is LangGraph? Key Concepts, Use Cases, and How to Get ...',\n",
       "  'url': 'https://www.designveloper.com/blog/what-is-langgraph/',\n",
       "  'content': '## LangGraph at a Glance\\n\\nLangGraph is an open-source library from the LangChain team and free to use. It can be used in Python as well as JavaScript environments, which means that it can be used by a large number of developers. In contrast to more basic chain-based systems, LangGraph is a graph-based system that coordinates AI workflows. This means that instead of a linear sequence of steps, an application is modeled as a directed graph of nodes and edges. [...] Imagine creating an AI application in which several language model agents collaborate on a complex task. It may be difficult to coordinate these agents, keep context across steps and achieve reliable results. LangGraph is a tool for solving this challenge. It is a framework on LangChain ecosystem that gives a way to define, coordinate, and execute multiple LLM (Large Language Model) agents in complex workflows. In other words, for anyone wondering what is LangGraph, it is a stateful [...] workflow continues. To put it briefly, LangGraph is about coordinating various AI elements to operate in coordination, have memory of previous interactions, and enable complex control flows within an application.',\n",
       "  'score': 0.917251}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_search.invoke(\"langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "870ad98f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m docs = \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLangGraph\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[32m0\u001b[39m].page_content[:\u001b[32m500\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/document_loaders/wikipedia.py:59\u001b[39m, in \u001b[36mWikipediaLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoads the query result from Wikipedia into a list of Documents.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        Wikipedia pages.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m client = WikipediaAPIWrapper(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     54\u001b[39m     lang=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m     55\u001b[39m     top_k_results=\u001b[38;5;28mself\u001b[39m.load_max_docs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     56\u001b[39m     load_all_available_meta=\u001b[38;5;28mself\u001b[39m.load_all_available_meta,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m     doc_content_chars_max=\u001b[38;5;28mself\u001b[39m.doc_content_chars_max,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:111\u001b[39m, in \u001b[36mWikipediaAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:126\u001b[39m, in \u001b[36mWikipediaAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m.top_k_results]:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wiki_page := \u001b[38;5;28mself\u001b[39m._fetch_page(page_title):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m doc := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_page_to_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwiki_page\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    127\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:86\u001b[39m, in \u001b[36mWikipediaAPIWrapper._page_to_document\u001b[39m\u001b[34m(self, page_title, wiki_page)\u001b[39m\n\u001b[32m     66\u001b[39m main_meta = {\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: page_title,\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.summary,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.url,\n\u001b[32m     70\u001b[39m }\n\u001b[32m     71\u001b[39m add_meta = (\n\u001b[32m     72\u001b[39m     {\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcategories\u001b[39m\u001b[33m\"\u001b[39m: wiki_page.categories,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m     84\u001b[39m )\n\u001b[32m     85\u001b[39m doc = Document(\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     page_content=\u001b[43mwiki_page\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m[: \u001b[38;5;28mself\u001b[39m.doc_content_chars_max],\n\u001b[32m     87\u001b[39m     metadata={\n\u001b[32m     88\u001b[39m         **main_meta,\n\u001b[32m     89\u001b[39m         **add_meta,\n\u001b[32m     90\u001b[39m     },\n\u001b[32m     91\u001b[39m )\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:475\u001b[39m, in \u001b[36mWikipediaPage.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    474\u001b[39m    query_params[\u001b[33m'\u001b[39m\u001b[33mpageids\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.pageid\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m request = \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28mself\u001b[39m._content     = request[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m'\u001b[39m][\u001b[38;5;28mself\u001b[39m.pageid][\u001b[33m'\u001b[39m\u001b[33mextract\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    477\u001b[39m \u001b[38;5;28mself\u001b[39m._revision_id = request[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m'\u001b[39m][\u001b[38;5;28mself\u001b[39m.pageid][\u001b[33m'\u001b[39m\u001b[33mrevisions\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mrevid\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[39m, in \u001b[36m_wiki_request\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    734\u001b[39m   wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n\u001b[32m    735\u001b[39m   time.sleep(\u001b[38;5;28mint\u001b[39m(wait_time.total_seconds()))\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[32m    740\u001b[39m   RATE_LIMIT_LAST_CALL = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev_Stuff/autoresearch/self_learning/automate/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"LangGraph\").load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d458cb6",
   "metadata": {},
   "source": [
    "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
    "\n",
    "\n",
    "== History ==\n",
    "LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc917ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import  Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int # Number turns of conversation\n",
    "    context: Annotated[list, operator.add] # Source docs\n",
    "    analyst: Analyst # Analyst asking questions\n",
    "    interview: str # Interview transcript\n",
    "    sections: list # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fd248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "588397e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: David Lee\n",
      "Role: AI Product Manager\n",
      "Affiliation: ProductAI Inc.\n",
      "Description: Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analyst.persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "791b2d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an analyst tasked with interviewing an expert to learn about a specific topic. \\n\\nYour goal is boil down to interesting and specific insights related to your topic.\\n\\n1. Interesting: Insights that people will find surprising or non-obvious.\\n\\n2. Specific: Insights that avoid generalities and include specific examples from the expert.\\n\\nHere is your topic of focus and set of goals: Name: David Lee\\nRole: AI Product Manager\\nAffiliation: ProductAI Inc.\\nDescription: Analyzes Langgraph\\'s impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\\n\\n\\nBegin by introducing yourself using a name that fits your persona, and then ask your question.\\n\\nContinue to ask questions to drill down and refine your understanding of the topic.\\n\\nWhen you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\\n\\nRemember to stay in character throughout your response, reflecting the persona and goals provided to you.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_instructions.format(goals = analyst.persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "867122a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an analyst tasked with interviewing an expert to learn about a specific topic. \\n\\nYour goal is boil down to interesting and specific insights related to your topic.\\n\\n1. Interesting: Insights that people will find surprising or non-obvious.\\n\\n2. Specific: Insights that avoid generalities and include specific examples from the expert.\\n\\nHere is your topic of focus and set of goals: Name: Michael Chen\\nRole: Business Strategy Consultant\\nAffiliation: FutureTech Consulting\\nDescription: Michael analyzes the strategic implications of adopting Langgraph for businesses. He focuses on how the framework can drive innovation, support digital transformation initiatives, and align with long-term business goals.\\n\\n\\nBegin by introducing yourself using a name that fits your persona, and then ask your question.\\n\\nContinue to ask questions to drill down and refine your understanding of the topic.\\n\\nWhen you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\\n\\nRemember to stay in character throughout your response, reflecting the persona and goals provided to you.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'You are an analyst tasked with interviewing an expert to learn about a specific topic. \\n\\nYour goal is boil down to interesting and specific insights related to your topic.\\n\\n1. Interesting: Insights that people will find surprising or non-obvious.\\n\\n2. Specific: Insights that avoid generalities and include specific examples from the expert.\\n\\nHere is your topic of focus and set of goals: Name: Michael Chen\\nRole: Business Strategy Consultant\\nAffiliation: FutureTech Consulting\\nDescription: Michael analyzes the strategic implications of adopting Langgraph for businesses. He focuses on how the framework can drive innovation, support digital transformation initiatives, and align with long-term business goals.\\n\\n\\nBegin by introducing yourself using a name that fits your persona, and then ask your question.\\n\\nContinue to ask questions to drill down and refine your understanding of the topic.\\n\\nWhen you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\\n\\nRemember to stay in character throughout your response, reflecting the persona and goals provided to you.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f822d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_question(state:InterviewState):\n",
    "    \"\"\"Node to generate the questions\"\"\"\n",
    "    \n",
    "    #get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    #generate the question\n",
    "    system_message = question_instructions.format(goals = analyst.persona)\n",
    "    question = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "    \n",
    "    #returen the question through state\n",
    "    return {\"messages\":[question]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05ca9508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c2f8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int # Number turns of conversation\n",
    "    context: Annotated[list, operator.add] # Source docs\n",
    "    analyst: Analyst # Analyst asking questions\n",
    "    interview: str # Interview transcript\n",
    "    sections: list # Final key we duplicate in outer state for Send() API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98b57bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"max_num_turns\":2,\"context\":[],\"analyst\":analyst,\"interview\":\"\",\"section\":[],\"messages\":[HumanMessage(content=\"hi do the proper search according to the experties\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4b7aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_num_turns': 2,\n",
       " 'context': [],\n",
       " 'analyst': Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\"),\n",
       " 'interview': '',\n",
       " 'section': [],\n",
       " 'messages': [HumanMessage(content='hi do the proper search according to the experties', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aecad88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generation_question(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a078187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='Okay, here we go.\\n\\nHi David, my name is Anya Sharma, and I\\'m an analyst focusing on the practical applications of new AI technologies. I\\'m particularly interested in Langgraph and its potential to reshape AI product development.\\n\\nGiven your role at ProductAI Inc., you\\'re likely seeing firsthand how Langgraph is being used. To start, could you give me a concrete example of a problem in AI product development that Langgraph solves particularly well, and how it compares to previous approaches? I\\'m looking for something beyond the general \"agent orchestration\" benefit â€“ a specific use case where it really shines.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='lc_run--6d35220d-ec9f-432a-9543-1387a45f2e5b-0', usage_metadata={'input_tokens': 235, 'output_tokens': 129, 'total_tokens': 364, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac1cb7",
   "metadata": {},
   "source": [
    "{'messages': [AIMessage(content=\"Okay, here we go.\\n\\nHi Carlos, my name is Amelia, and I'm an analyst focusing on the practical applications of AI tools for non-technical users. I'm particularly interested in Langgraph and its potential to empower citizen data scientists.\\n\\nI understand you're an AI Product Manager at DataWise Analytics with a focus on Langgraph's user-friendliness. What are some of the biggest usability hurdles you've observed for citizen data scientists trying to adopt Langgraph? I'm looking for specific examples, not just general challenges.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--091faa01-db5b-49db-bc14-535069af68ec-0', usage_metadata={'input_tokens': 234, 'output_tokens': 115, 'total_tokens': 349, 'input_token_details': {'cache_read': 0}})]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f390e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here we go.\n",
      "\n",
      "Hi David, my name is Anya Sharma, and I'm an analyst focusing on the practical applications of new AI technologies. I'm particularly interested in Langgraph and its potential to reshape AI product development.\n",
      "\n",
      "Given your role at ProductAI Inc., you're likely seeing firsthand how Langgraph is being used. To start, could you give me a concrete example of a problem in AI product development that Langgraph solves particularly well, and how it compares to previous approaches? I'm looking for something beyond the general \"agent orchestration\" benefit â€“ a specific use case where it really shines.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47bdc1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2fb6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query writing\n",
    "search_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation. \n",
    "First, analyze the full conversation.\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "Convert this final question into a well-structured web search query\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9f8bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(state:InterviewState):\n",
    "    \"\"\"\n",
    "    Retrieve data from the web\n",
    "    \"\"\"\n",
    "    structure_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structure_llm.invoke([search_instructions]+state[\"messages\"])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "826831e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"max_num_turns\":2,\"context\":[],\"analyst\":analyst,\"interview\":\"\",\"section\":[],'messages': [AIMessage(content=\"Hello, my name is Alex Thompson, and I'm an analyst interested in understanding the strategic implications of adopting Langgraph for businesses. I'm particularly keen on how this framework can drive innovation and support digital transformation initiatives. Thank you for taking the time to speak with me today, Michael. \\n\\nTo start, could you explain what Langgraph is and why it's becoming a significant consideration for businesses looking to innovate?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 224, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CPRmMT7ufhFyYLhMtNpeguI9W2y6O', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--827b799b-ccb9-422c-a444-402d7ddc4550-0', usage_metadata={'input_tokens': 224, 'output_tokens': 79, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "839a5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_web(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf690c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document href=\"https://www.ema.co/additional-blogs/addition-blogs/building-ai-agents-langgraph\"/>\n",
      "When youâ€™re building AI agents that need to be reliable, adaptable, and aligned with enterprise workflows, LangGraph gives you a strong foundation to do just that. The following are the key benefits you can expect when using LangGraph for your AI initiatives:\n",
      "\n",
      "1. Built-In State Management [...] By now, youâ€™ve seen how LangGraph empowers you to build structured, intelligent AI agents that are not only adaptable but also enterprise-ready. From graph-based workflows and persistent memory to multi-agent coordination and human-in-the-loop controls, LangGraph brings the modularity and transparency todayâ€™s businesses need to automate with confidence. [...] ## What is LangGraph?\n",
      "\n",
      "LangGraph is an open-source framework designed for building stateful, multi-actor AI agents using graph-based computation. It enables you to define complex workflows where each node in the graph represents a function, a language model call, or a decision point.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://docs.langchain.com/oss/python/langgraph/overview\"/>\n",
      "LangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n",
      "   Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n",
      "   Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point. [...] Trusted by companies shaping the future of agents - including Klarna, Replit, Elastic, and more - LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, it is recommended you familiarize yourself with some of the components used to build agents, starting with models and tools. We will commonly use LangChain components throughout the\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03\"/>\n",
      "While the framework introduces some complexity and requires investment to master, its benefits for developing advanced AI applications make it a valuable addition to the LangChain ecosystem. As LLM applications continue to grow in sophistication, tools like LangGraph will play an increasingly important role in helping developers create more capable, flexible, and reliable AI systems. [...] The most significant advantage of LangGraph is its explicit support for cyclical workflows, which are essential for implementing agent-like behaviors. As emphasized in the documentation: â€œCrucially, this is NOT a DAG frameworkâ€¦ Cycles are important for agent-like behaviors, where you call an LLM in a loop, asking it what action to take next.â€ This capability enables the development of sophisticated agents that can iteratively refine their understanding and actions, leading to more capable AI [...] > LangGraph is a powerful Python library designed for constructing stateful, multi-actor applications with Large Language Models (LLMs). It extends the capabilities of LangChain Expression Language (LCEL) while specifically addressing limitations in existing frameworks for agent development. As a specialized tool for creating complex LLM applications, LangGraph provides a structured approach to building sophisticated workflows that require cyclical processing patterns.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.datacamp.com/tutorial/langgraph-tutorial\"/>\n",
      "For applications requiring autonomous decision-making, LangGraph enables the creation of agents that can perform tasks independently based on user inputs and predefined logic.\n",
      "\n",
      "These agents can execute complex workflows, interact with other systems, and adapt to new information dynamically. LangGraph's structured framework ensures that each agent operates efficiently and effectively, making it suitable for tasks like automated customer support, data processing, and system monitoring. [...] ### Workflow automation tools\n",
      "\n",
      "With LangGraph, automating business processes and workflows becomes straightforward. Intelligent agents can be designed to handle tasks such as document processing, approval workflows, and data analysis. By defining clear workflows and leveraging LangGraph's state management, these tools can execute complex sequences of actions without human intervention, reducing errors and increasing productivity.\n",
      "\n",
      "### Recommendation systems [...] Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\n",
      "\n",
      "LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.langchain.com/langgraph\"/>\n",
      "Principal SWE\n",
      "\n",
      "â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€\n",
      "\n",
      "Andres Torres\n",
      "\n",
      "Sr. Solutions Architect [...] Principal SWE\n",
      "\n",
      "â€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€\n",
      "\n",
      "Andres Torres\n",
      "\n",
      "Sr. Solutions Architect [...] Other agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a companyâ€™s needs. LangGraph provides a more expressive framework to handle companiesâ€™ unique tasks without restricting users to a single black-box cognitive architecture.\n",
      "\n",
      "Does LangGraph impact the performance of my app?\n",
      "\n",
      "LangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\n",
      "\n",
      "Is LangGraph open source? Is it free?\n",
      "</Document>\n"
     ]
    }
   ],
   "source": [
    "print(result[\"context\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86a64aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(state:InterviewState):\n",
    "    \"\"\"\n",
    "    Retrieve data from wiki\n",
    "    \"\"\"\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    print(\"*******************************\")\n",
    "    print(search_query)\n",
    "    \n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8edfb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"max_num_turns\":2,\"context\":[],\"analyst\":analyst,\"interview\":\"\",\"section\":[],'messages': [AIMessage(content=\"Hello, my name is Alex Thompson, and I'm an analyst interested in understanding the strategic implications of adopting Langgraph for businesses. I'm particularly keen on how this framework can drive innovation and support digital transformation initiatives. Thank you for taking the time to speak with me today, Michael. \\n\\nTo start, could you explain what Langgraph is and why it's becoming a significant consideration for businesses looking to innovate?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 224, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CPRmMT7ufhFyYLhMtNpeguI9W2y6O', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--827b799b-ccb9-422c-a444-402d7ddc4550-0', usage_metadata={'input_tokens': 224, 'output_tokens': 79, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b83fd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "search_query='Langgraph framework benefits for business innovation'\n"
     ]
    }
   ],
   "source": [
    "result = search_wikipedia(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83262f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_docs = WikipediaLoader(query='Langgraph framework benefits',load_all_available_meta=True).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82687a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5bf9f74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['']}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec30a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "        \n",
    "1. Use only the information provided in the context. \n",
    "        \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "        \n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "        \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a281deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state:InterviewState):\n",
    "   \n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer question\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "            \n",
    "    # Name the message as coming from the expert\n",
    "    answer.name = \"expert\"\n",
    "    \n",
    "    # Append it to state\n",
    "    return {\"messages\": [answer]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ad150",
   "metadata": {},
   "source": [
    "how many analyst we were doing to be create:\n",
    "4\n",
    "\n",
    "max_trun:2\n",
    "\n",
    "means if atleast 2 expert are giving ans then we can save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d88f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_messages(state: InterviewState, \n",
    "                   name: str = \"expert\"):\n",
    "\n",
    "    \"\"\" Route between question and answer \"\"\"\n",
    "    \n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns',2)\n",
    "\n",
    "    # Check the number of expert answers \n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # End if expert has answered more than the max turns\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # This router is run after each question - answer pair \n",
    "    # Get the last question asked to check if it signals the end of discussion\n",
    "    last_question = messages[-2]\n",
    "    \n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return 'save_interview'\n",
    "    \n",
    "    return \"ask_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f9a6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_interview(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Save interviews \"\"\"\n",
    "\n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Convert interview to a string\n",
    "    interview = get_buffer_string(messages)\n",
    "    \n",
    "    # Save to interviews key\n",
    "    return {\"interview\": interview}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c8306476",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "            \n",
    "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "        \n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "        \n",
    "6. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "7. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "        \n",
    "8. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a50dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_section(state: InterviewState):\n",
    "\n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    interview = state[\"interview\"]\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "   \n",
    "    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Use this source to write your section: {context}\")]) \n",
    "                \n",
    "    # Append it to state\n",
    "    return {\"sections\": [section.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ce2d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_builder = StateGraph(InterviewState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "72dde8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x127221050>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_builder.add_node(\"ask_question\",generation_question)\n",
    "interview_builder.add_node(\"search_web\",search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\",search_wikipedia)\n",
    "interview_builder.add_node(\"generate_answer\",generate_answer)\n",
    "interview_builder.add_node(\"save_interview\",save_interview)\n",
    "interview_builder.add_node(\"write_section\",write_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ac2c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x127221050>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\",\"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\",\"search_wikipedia\")\n",
    "interview_builder.add_edge(\"search_web\",\"generate_answer\")\n",
    "interview_builder.add_edge(\"search_wikipedia\",\"generate_answer\")\n",
    "interview_builder.add_conditional_edges(\"generate_answer\",\n",
    "                           route_messages,\n",
    "                           [\"ask_question\",\n",
    "                            \"save_interview\"])\n",
    "interview_builder.add_edge(\"save_interview\",\"write_section\")\n",
    "interview_builder.add_edge(\"write_section\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f59d9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name = \"Conduct Interview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ac2781b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAJ2CAIAAADNP57VAAAQAElEQVR4nOydB2DTRhfHT7Kzd8gim7AhQAibj7L3KLPsvWeZZe+9oewCpUCBAmVDgbKh7BkIe68sIHvHsfQ9W4lxEjuJwVZs6/2aGunudFp3f717dzqJWZYlCIIg2kNMEARBtArKCoIgWgZlBUEQLYOygiCIlkFZQRBEy6CsIAiiZVBWEH0indw6GxP2LjklQSqRMOkpLKFYwlKyKPkCRROGZSlZCAtBsMoy8C+hRIRNl+dAE8IoLUBSimKZzPxpIt+SJQylvFtZPopALm9uQbYnpc1lOTCEoZW3FZkSkQltZiF2djctU92mUGFTIngoHLeC6ANHfguLeJ+SnsrQYsrMQmRiRovEbFoy87WS0xQoCkVTsjWG5cJlq/JlmqYYqSwmQ2hIZpRMR+QLHPJV2JTJzIEDEsMvl+xrDqBHGbKiVEdEFJFmqTJiM5phiCQVjlaaliqFreydTRp1K+zqLVx9QVlBCpg9yz5+DkmxtBEXLW9Tp30hYuDcPRvz8GpsXLTE2k780wgvK0cRER4oK0iBce9c3LXjn2wdTdoP87SwM7bqd3hd2PsXCR5FLdsN9yACA2UFKRgOrgkFI6VRF7ci5S2J8fL7lDfQbuo325cICZQVpAC4+k/U05vxfWf5EAFw9LewqE9pvaYJ4mQ5UFYQvvl7xYe4aKmgHuCH1oV/DkkeMK8IEQY0QRAeOfXnp9iodKE1CtoMdXPxMvtj1jsiDFBWEP5IiGJeBMX3nyOUh7YyrQe7SyXMya0RRACgrCD88dfSd8UrWBOh0nWCz8vgeCIAUFYQnrhzJi5dyjbu6UqEiqUN7eBs+teSD8TYQVlBeOLO+UifUlZE2LQe6BkVnkqMHZQVhA8SYhhJirR5H15Nlb17986YMYNozsSJEw8fPkx0gLUjbWJGn9xm5B4WlBWEDy7uizC35Pu91sePH5Nv4ps3zA+eRa1C3yQTowZlBeGDTyGpTh5mRDe8ffsW7ItGjRo1bNhwzJgxQUFBEDhw4MBjx479888/lStXfvr0KYTs2bNn+PDhdevWbdKkyaRJkz5+/Mhtvnv3bgi5cOFC1apVly5dCulDQ0PnzJkDKYkOKPs/25REKTFqUFYQPkhJknqX0Mkg/bS0NFAQkUi0evXq9evXi8Xi0aNHp6SkbNy40d/fv0WLFrdv3y5VqhRozZIlSypUqADCMWvWrKioqKlTp3I5mJqaJiYm7tu3b/bs2R07drxy5QoETps2DYSG6ACfUhaEJVGh6cR4wflWED5gGeLuZ0F0wLt370AjunTpAtoBqwsXLrx79256evZKW65cOXC1eHt7g+7AqkQiAfWJjY21s7OjKApkqFevXlWqVIGo1FSdu1RpMfXheZKjuy0xUlBWED5gGdbJXSfzj4BSODg4zJw5s3nz5pUqVQJ7BFoxOZOBOQOtnmXLlj18+BBsEy4Q9AhkhVsuW7Ys4QuaZmKi0ojxgo0ghA9kU7npZuYDMzOzTZs21apVa9euXf369WvTps3x48dzJrt48SK4XcqUKQOJb926tWbNmmwJoClE+IJlaYox5nfxUFYQPqAIiY7QlTfB19d31KhR4KBdvnx5sWLFpk+fzvlolTl48GBAQMCwYcNKlCgBrZ74+IIc7cpKiZWdCTFeUFYQPqBoKuR1EtEB0A105MgRWDA3N69du/aiRYvAe/LkyZNsycCN4uLiolg9d+4cKTikUsa9mDHPMoOygvCBmQX94ZlOZAX0AnpwVq5c+eHDB3Df/vHHH+CvBQ8LRHl5eYEnBZo84EMBI+X69evQKwSxO3fu5LYNCwvLmSG0qkCAFImJton8IAGZLVzEmGe6RVlB+MDe2Sz8nU7GgIGCTJ48+cSJE23btm3fvv29e/c2bNjg5+cHUe3atYP2DjR8Xrx4MXTo0Jo1a4J7pUaNGuHh4dDHDH6Wn3/++eTJkznz7Nu3L4jR2LFjk5O1f8x3zkWJTSli1OA0TggfhLxIPrg+ZPjyYkTwbJr62tnDvM0Qd2K8oLWC8IFHcQtTc9HF/Z+J4ElJlLbqb8yaQnDcCsIbxcpbPb4RV6e9s7oEEyZMuHHjhsoo8HFww9hyMnPmTB2NsgfU5SyVSsHMV3dIZ86cURe1e9kHS2uxyJh7gWRgIwjhj7XjXgbUdvjfj6o/BhQZGaluhCuEgydVZZSjoyP0ARHdEBoaqi4ql0Nyd1drjKwe/aLPVD/rQkb+8SC0VhD+qNfB7fzf4epkpVAhvfv2WC4C8Q1sm/PezcfS6DWFoG8F4ZMy1a2dPc13zH9PhMelg5GpSdKfRgniU2QoKwivdBztKZEwf6/8SITEy6DEh1djBi4Qytzg6FtBCoB9q0LSkpmuE7yIALhzKvbm2S9DFhUlggFlBSkY/pz7Lj2d7TPTlxg1e1d8jI5IHbRQQJpCUFaQAuToxrD3TxO9S1u1GlCYGB03T0bfOhNpaSPuM8OXCAyUFaQgSYiS7ln5ISUx3dXLokZLJ49iupqYkjckqeTfP8NDXiQxLBtY37FaUwciPFBWkILn9cPkq0e/xH5Jo0WUuSVtbSe2tBObmJDUFEY5GUXLZpmjKMKVWZomDMOFUyzDwraMNKMw0zTFcBOaUPL0mdmIxLQ0nZFN08Bm5KbIh5b1XmRsBcsQL8+TMFKi2KU8DZElUToAsRnNppOkeGl8tCQpIR2OwdLKpFRV2//96EiECsoKokc8+C/+7cOEuBiJJAWqJytJyVo4OTnIlBXFApRiwlIZEkCyRmVdJpSUkVI0JyFZ86Hkb/8pQmSLLKWQHkVWijSKDUH+KBElEtGWtiKPohbqRuUICpQVREAcPXr07t273/bxICT/4ChbREDk8m4RokXwEiMCAmWFH/ASIwICZYUf8BIjAkIikZiYGPusBHoAygoiINBa4Qe8xIiAQFnhB3yDGREQUqkUZYUHUFYQAQG+FZQVHsBLjAgIbATxA15iRECgrPADXmJEQKCs8ANeYkRAoKzwA15iREDgcDh+QFlBBARaK/yAlxgRECgr/ICXGBEQKCv8gJcYERDoW+EHlBVEQKC1wg94iREBgbLCD3iJEQGBssIPeIkRAYGywg94iREBAbKCLlseQFlBBARaK/yAlxgREIUKFRKJRATRMSgriICIjo5OS0sjiI5BWUEEBLSAoB1EEB2DsoIICJQVfkBZQQQEygo/oKwgAgJlhR9QVhABgbLCDygriIBAWeEHlBVEQKCs8APKCiIgUFb4AWUFERAoK/yAsoIICJQVfkBZQQQEygo/oKwgAgJlhR9QVhABgbLCDygriIBAWeEHlBVEQKCs8APKCiIgUFb4gWJZliCIUdOiRYuwsDAo6hRFcSGw7O3tffjwYYLoAJogiLHTqlUrmqZFIhGdCZgtrVu3JohuQFlBjJ/u3buDbaIc4uXl1bZtW4LoBpQVxPixtrYGEVGec79evXoODg4E0Q0oK4gg6NSpk7u7O7cMCx06dCCIzkBZQQSBqakpKIuZmRks16hRo3DhwgTRGdgThHwXt07GRH1KTUuVZqzTFGFYWkQxUlm5omjCMvAPdL0QSkTYzFSU7HEmC5XFyhGJiFQeK0/LJSAZsbDMQIi8rGaWVhpWKciQpWnCZGYi25iSJc7IBf6FNOzXbW/fuS2RSMqXL29laSWLFVEQodg842hlmcvyIBnLFMOwmTuV7UuWG8NyOX+FO0cuUClO+Qp8TSvPgdtKlpZRcWFNTGgre9OaTRxFFsTgQFlBvpFzuyOf34mlTShaRCQpGaWIpVmKoRT1U1ZrofbLKxVNQwXO6N+V139ZhKJGUSKWlVIZUWyW+pZRSWnQBepr5aRlogQhin1x28q1SqkaZ9R2mbYQmbDIyjsoRWasPCn7dXOFGBGSoQzK+XPLyif1lczD/iouOcKVEsvPhSjpYA5EcGFpkprCOLiYdR3vSQwKlBXkW7h7Jvb22ehG3d2dPE0JoksOrPpoaUv9NNKDGA4oK4jG3DwRG3Q5ust4X4LwwrHfPoKh1Hmcwdgs6LJFNCb4SnSxCnYE4YuWgzyjIlKJ4YCygmhMaprUvyYO+uAVE1P62rFoYiDgq4aIZkjTCJPOWNgQhE+k6WxygsG8JImygmgMy1IE4Rfo5JZKDcYNirKCIAaArGfFcDpXUFYQDUFLpSCAniDKcByhKCuI5uCYBP7hRvIZCCgriIawaLAUAPJGkMFcd5QVBDEAKIpQhqPmKCuIZuBIpwKBxUYQYsQwFPpWCgCapigRMRRQVhANQd9KQcAwmW94GwIoK4iGoKYUBBRlSFceZQXREGwBFQQ4HA4xamTPTJQWvqFFhDKcyop+fURDKMKDOb7/wO4GjaoSvYe342SkhDWczzGirCAawhCBc/DQ3gWLZnDLZUr79+jen+ge9K0giDHz7NljxXLp0v7wR3QP+lYQo+abnpkHDu65fv2/J08empqZVSgf2K/fMA932RSKLMvuP/DXv/8e+/DxnY93kcqVq/ftM0QkyjJCQyqVTpg4IjwibO2arXa2uc1Kt+G3X0+d/ic6Oqp5s9Y/1Ko3acqofXtPFirkBAsQu2DeSi4Z7G7h4pn/HL1kaWmZnp7++5Z1129c/vQp3N8/oG3rjtWr1+KSvX//9o+tG4Lu34GDLFu2fOeOPcuVCxg1ZuD9+3ch9tSpf37bsCM4OGjd+uVnT9/kNtn+5+Z/Tx378uWTi4tbQIVKo0dNomlZg6BNu4Z9eg+OjY3Ztn2jhYVFlco1hg8bBwdG8g34VojhjFvBRhCiMZoKC9S91WuWlC1bYfbspRMnzIJqP2/+VC7qwIHdO3Zu6dC+6+5dx1q1av/P8UO792zPtvnipbOfP3+yeNGa3DXl2D8H9+3fNWrkxMOHzpUpU2712qUQqPwlQ5WsWr0YtmrbptOunUfr1G4wY9b4i5fOQnhaWhooCAjcooWrly1ZLxaJp0wdnZKSsnL5RjBPGjducf7s7RLFSylnBRp06PDeIYNG7fv73359h164ePrvfTu5KBMTkz17toPEHDp4dtsf+4MfBm3d9hvRBBZ8KwyOskWMFVZjYxwq+R+/7/X09OYqebpEMnnq6Ni4WJCJ+w/ulixZpkmTlhDeskXbihWrJCclKW8Lz//z508tX7rBvXAeU8+fOHkELJTaP9SH5RbN2zx+HBwa+jH3TVJTU8G46Nql94+t2sMq2DgPH97f/ucm0JcPH96B/LVv14XTjhnTF8KhgmmjLqv4hPi/dm8bMnh0rVp1YbVunYavX7/YsfP3dm07g6ZAiIeHV/dufWVJrW3AWgGhJJrAcl9AMRDQWkE04xteeINnPtTwSZNHtvyxTr0GlUFTIDAmOgp+/f0r3LlzY/GS2Sf/PQpCAy2jYsVKyPci48zZk2ACTJ40B5LluZeXL5+BQilWQcsIyeM9GqjbYJVAJVeEQMvl9euXcCQggvb2DtBWAmMKtAYMjYoBla2trdVlBTIkkUiU/Swlb9JNbwAAEABJREFUSpROSEgICfmgWFVE2djYJiYmEE1Aly1izHzD+25XrlycOn1st659Bg0cWbRo8dt3boyfMJyLguaPpaXVlasXFy2eBbZM3bqNBg342cnJGeQAXCoL5R0u5mbmee4iMTERBMLCwlIRYm6e99cAExLi4XfEyH7ZwqOjIn19/X5dsQkaZdBEAueLu7tn754DGzVqri6rqKgv2Q6VO5jk5Azji/q+F5DRZYsYNZrXjmPHD4Kzs3+/YdwqV5k5wAqAtg/8vX37+u7dm1u3b4TH+Py5K7jYsWOmQNMDTAZoQzk4OOayC3C+gk2UmpqiCFHU55xImYyPthZycub2Ai0U5QTgcIVfb2/fIYNHgasVDgxaWPMXTvfx9cvmT1FgZSUzZJJTkhUhSUmJ8OvoqIFfNhdkrxqKDaZvHxtBiIZo/syMi4t1dnJRrP733znFMnTKvHnzChbAOmjXrjP4MqAtw0WB4jRr+uPIERMsLSwVLl51gC3g5uau3PX7IPieYtnUxJSr5BzQYOEWPD28uY+9QwOH+/P18YMOKRAp6AYCKSEyq8e8Zs3aM2csAmMqF4dI0aIlQNcePbqvCIFuLxtrG2dnF6INZK8aphtMbUVZQTRE84kRihUtcev29XtBt8HlqegcgQ5j+D177uT0mb9cvXoJ3BnXr1/+7/I5/7JZ3CjQHTtz5mLo5d37947c9wJe0nPnT0E/TlJSEvRn37x5VREFLo+nTx+B0wSWoQl2+coFLhzko3evQeCjhb4qaEPBtuPGD13560Iil0Lw+KzfsPJjyAeQoZ27/oCD544NTBuQjLv3bkXL3UMctja2jRo2B0cMnEtcfBx0Px88tKdDh25cB7PQwEYQoiGa+w779h0KxsLUaWOSk5OhZwT6mMPCQiZO+nnK5Lljx0xds3bplGljiKy9UAhaQz916J5tc2h39OwxYNPmNZUrVffzK6ZuL9279YuM/PLrqkVQ2yEZdLusXbeci2rTuiNYHwMHdwN/Tf16jbt37QsNK86b27lTTzA0du3eCi0daMiULVN+7FiZZQRO4jGjJ0M3MCdnlStVW75sA5hUsNyqRTswW34ZPwz6npUPYNjQsSAic+ZNBgECX0zXLn26dO5FtIRhuWzxG8yIZkjTyLoJL3vPLEb0m/MXTs+eM+ng/tPQoUMMn+1zXpUIsGnUXTtNKl2D1gqiGTjdSoFAUSx2MCNGSwEat5OmjHoYHKQyqnnzNtBrQ4wX9ht8WgUHygqiGSLw8xfQY3PcmKlpkjSVUZZKI1Y46tVtBH/EaMAPeiBGjBQ6OwvoqanRu3lIAYKygmgG+laQPEFZQTQDOw4LBHwnCDFmDOjbekaGrDPIQEBZQTQDrZUCQfZVQwZdtgiCCBWUFURDsBFUEKBvBTFmPr7/SBDewflWEGMjOjr6ZiYmlFm9ogsJgqgHZQVRTVpaGojIrVu3bty4ERkZWVVO//79XQoVXjfhJUEQ9aCsIFm4d+8eSAkIyuPHjzkpmTt3brFiSu8rSwmFs/Twjqm5CP6IgYCygpDnz59zUgL4+/tXqVJlxIgRFSqomZVaRMRiUdjLtMLFTAnCF9J0prBv3nP66gkoKwIlJCREISUuLi4gJZ06dVq6dCn39YncsXEUB136XLiYB0F44emNeIqiSlS2IgYCTuMkIGJjYzkdAUGB+w5SwjVz7O3tiYb8Nv515WaFSwTmPbs98v3snP+mRjOnCnVtiIGAsmLkpKenK6QkIiJCISUeHt9ra2yc/MbK1sSrpJWdsxkjVfouF0Vl++oHC49aRQgXmyONyg0pimbZHNPN06o+Ly9LCsFZN+e+2pVjV7JR8Cyd8zNq8sPMngPJGchlqJQvxX0ajFuV71XF+SmCqCyzp8i25RazbkOJKUkyefso/ktISpdffOydDedTqSgrxsr9+/c5KXnw4AGnIyAoJUuWJFpl/+qwqPAUqYRNlyhV9GzzDVHcRCF5FzOWYvPz4T6ViiSrnTm+YcRm6EKOnVOZ0dkzYVXMaaJyczZruHKGOWNzO271O6JYhmIk6bFvYg98SXkLkgdPiMTExOPHjxO9B2XFeHj16hV0BnMek1KlSnFSEhgYSJBMjh07dvv27ZkzZxL9ZsaMGf/++y/08XOrNE0zDMMZU3fv3iV6D7psDZvw8HDFQDVHR0eQknbt2i1cuJD7/A2SDXjg5/mxd31g1qxZycnJ58+fVzz1QVkMRVMIyoohEh8fr5ASiURSrVq1WrVqjRkzBmSFILliKLICLF68uFu3bs+ePVOEgLKA1lhYGICbHGXFMAAbWDHmFfqGOXdJ9+7dvby8CJJvQIXz04OuJyxfvnzQoEEfP2a8hNWvX79x48atXbs2Li7O1taW6DEoK3pNcHAwJyX37t3jpGTq1KngNyHIN2FA1grg6uoKt3v69OmfPn0Cx8rgwYO5cHAP7du3b8qUKd/fnacjUFb0jtevXysGqhUrVgykZODAgZUqVSLId2NYsgJUrly5Z8+eGzZsUG771K9f39raGjz0ICuXLl2qXbs20TOwJ0gviIiIUEgJ2LfQgwMeE/g1iIa0AbFp0yZoTkLLghgLu3fvXr169enTp6GoUHozISjKSoGRmJioGKgGrjjFQDUnJ/xsha5Yv369qakpOCmIEZGamgpaCb3R4OUFw9bHx4cUNNgI4huFlLx7946Tks6dO+tDURAC0AiysjKYN2vyCTeYAKwVaA3t2rVr0qRJ4NQvWLcLygofPHr0iJMSgJOS8ePHly1bliD8YnC+FY1oIofI3yMdMGDAihUrtD6uOp+grOgKMEYUY159fX1BSnr37r1u3TqCFBzGLSsKoLBt3bo1LCyMyJ0vP/zwA8/GC8qKNvny5YtioJqlpSXc3ebNm8+cOdP4DG8DRSCyArjIIfJe6qFDh0LjCNpKvJ07ysr3At5WhZTEx8dzPThwI7mbiugVEolEILKioJ4cOHHoIhg2bNiIESOgiBIdg7Lyjdy+fZsbqPbq1SuuB6dDhw5FihQhiB4jHGslGyYmJnZ2dlOnToWHH8gKOPugrIJBTXQDyooGPHnyhJMS+A0MDAQpGTdunL+/P0EMBMHKCkcpOUR+HZo2bQo+XR0Ns0RZyYP3798rBqp5enqClPTs2XP16tU0jfNEGx4ClxUFFSpUuHTpEvQqwDKIS/Xq1WvUqEG0B15iFURGRiqkxNTUFHwl0G8HBqSNjcHM+oeoBGVFGW60VOPGjTds2FC+fHl4UmprVDde4gxSU1MVA9Wio6O50SUDBw50c3MjiLFgWG8w80PZsmXB+mYYBny6tWrVmjRpUosWLcj3IXRZuXv3Liclz54946Rk3rx5RYsWJYgxgtaKOsBUAWP87NmzFy9ehNUrV644OjqWLl2afBNCvMSgIJyUwC/YfiAlP//8s9rP4iBGBMpK7piZmUGbCBY8PDymTZs2ZMiQmjVrEs0RyiUOCQlRjHmFdg33Jg44q0QiQ5rQHPlOUFbyia+v759//glORlgeO3Ys9Ht269Yt/5sb8yWOiYlRDFQDGw+kpH79+hMnToQOfIIIEvStaEShQoXgFzortm7dmpycDFcvKSkpP95GY5MVOHOFlHz+/Jkb89q3b193d3eCCB60Vr4BBweH0aNHE/nMp/3792/Xrh1UqNw3MZJL/OTJk//++w+k5NGjR9yY19mzZxcvXpwgiBIoK9+DlZXVsWPHgoODifzTKOBAaNasmcqUxnCJ79+/v2zZMugbGz58eEBAAEEQNYDLAGXlOylXrhz81qhRA57cRYoUUTmzsjFc4k+fPoHjeuDAgQRBcuXdu3fQTCbIdwNul19//VVdrDGMQIfnDxi3BEHyAouKFgkLCwsNDVUZhbKCCAgsKlrk6NGj4GFRGWUMjSAsK0g+waKiRaB3Vd0E+ygriIDAoqJFWrZsqS4KG0GIgMCiokXQt4IgMrCoaBH0rSCIDCwqWgR9KwgiA4uKFkHfCoLIwKKiRdC3giAysKhoEfStIIgMLCpaBH0rCCIDi4oWQd8KgsjAoqJFjNy3YmJigq+lIvkBi4oWQd8KgsjAoqJF0LeCIDKwqGiRXHwrlDq90X9GjRp18eJFiqK4VcXCnTt3CIIo0bBhw5iYGKlUyhUS+GUYxs3N7cSJEwT5VsC3AuqhcpZoA/atrFy50sfHh86EkuPl5fXhwweCIEo0btwYdEQkEikXlTp16hDkO8jFt2LYLtv//e9/ytYWLFevXh2UhSCIEp07d+a+N6wACkmXLl0I8h2AnVK4cGGVUYYtK926dfP19VWsQlnp2LEjQZCseHt716tXTzmkWrVq2YQG0RTwrbRq1UpllGHLCuhlgwYNuC8TgpVbsWJFPz8/giA5ANtEUTag2LRv354g34cxj1uB4gLPIiI3VX766SeCIKpwdnZu0qQJ9wQqX758yZIlCfJ9fO+4lbeP01ISUxSr4O5iiew/Am51luuCUXJxyKKzdi9R0OFEZdmAIbIA5eTyGEV6WSrCbZI1Dfyfmbe84wfSiBpW73U28WyZ4qXpJM+nt+Lk21GKfGBj2ISS/6e8l8zDpOTxbJazk61l7SCTJ8pyatTXI5FBy1eznoICkcikeKAFMRyiPko/RSSDBagcSNGEZXIkhRNnst7trBcm91C4TNnKC3hUGUZF76TqDEiWcqIMS7MUQymHVC75Y1CpiOSUlLqBnWXlRB0UIep7R9nM8qF2a5piGVbt4WakUXUlc+6LJlQ+kgEWlmY+Zc0Iv+QybiWPDuZ9K0O+hKbChU5Py3p+WS+9vB5/vYWs/Far2lvGVvIaStRmqCQHWUJYQrJulW2/mYE5gnLfY7ZiJFcZSkUeajMn8oNlWDXbgHib0lDULG3Fvabpe3v+7F9fXgXHSyVwvCwrzaatKuqbiguikPU8oYjiWfM1Q/kTgOQfdSqgXh3Uls+MDXNThDy2VVyQPLQpj0wyjyS3TJQRmUCzg3UqbPHTaL34KHBusrJ7cQhDmOotXJ09TQnynaSRc3+Hh71LGrxIf70/d8/FBV2KLlfTsVQ1G4IYFJ8/pt345xNIYudfPAkv5DJuRa2sbJ/7nhbRrYfydIgC4eW95JsnwwYt1EdlOb3z85vghC6TihDEYDm0LoRIpT2mehPds3HjRvhV+TVR1S7b50HJyQkS1BStU6yihYWN+NC6MKJ/vAqO+6GdG0EMmTZDPRIT0l/eSSa6J5dxK6pdto+uxlpZ8+0BEgjuPpbvXyQQPePp7WTwEHqWNCS/MqISK2uTh9diilXS+a3UeL6VpPg0IsqfDxrREDMrOi1FSvSM2M8p+XUPInoOzSQmphHdo/G4Fej3kaShrOgEKcuk69+MH1JGmi5BWTEGpOksP7fSyOdbQRBEAUvy13v93Rj5fCsIgigQiShak3E/30wuvhXVsgLHxcuBCRN9vLiy2QJobAQZA0wu4zK1isbzrciGWDJYyHQEo4czZzGyO7rnipcAABAASURBVI6PEuOAIrw8uDT3rWABExhonRoPfD21NPat0CIsZTpEDy+uwU49iuSA4mmogMbjVph0lpFiQdMVenhlKQoNFiOC5eNeGvl3ggwNSg8rMMtS+ujyQTSHzZi1ROfguBU9Iuu8LPqD2rkgEMNCZnjyYi1o7FtBk1h3yHr/WL27uKgpRgPL8PTUMvJvMBsaetnWQFUxFnizCTT2rRhKI3ve/KkjRvYjPLL/wO4GjaqS70MfqzDLGoSyHPvnYL0GlbX+ZcLXr19Ctg8e3IPlGTPHjx03JGcadeHfTJ9+HVf+upBoqVB9ha+Ghsa+FflstYiu0EfPChH0C8z29g49e/R3ccltupnatRtIJDp5M7hMaf8e3fsTrcHTE0Jj34p8IlOCCAdK2M4VR8dCfXoPzj1Ng/pNiG4oXdof/ogWKehxK1rrCXr//u0fWzcE3b8DglS2bPnOHXuWKxcA4WCv/r5l3fUblz99Cvf3D2jbumP16rW4Td68eXXk6L67926Fh4f6+vg1b96m9Y8diNwi7Teg84J5K5cunwuPkc0b/4LAa9f++3X1os+fPxUrWqJNm47Nmv7IZWIiNgkKujNvwdSYmGiIGjFifJlc71C7Do1b//hTr54DYDk2NqZNu4Z16zScMX0hF9uhY9P27bp06dzr0aMH27ZvfPr0kZ29Q43qP/TqOdDKyopLA6ZcaFjIli3rbty84uTk0qVTr8aNWxAN0M8OZo2LYnxCPNzxG9cvR8dElSxRpmHDZi2at+GiTv579MjR/W/evCxSpFj9eo3hknLnnJCQ8Pe+HTdvXXv79lUhR6eaNev07TPE3Nwcolq3bdCze/9Ll89BS+TwoXO2NrZQopatmAer7oU9fvihPqQ0Nc2YUzky8suceZPhHnl6enfu1FOxX5XMmTs5Ojpq+bIN3GqvPh2gqBw+eFYRm5iUOLD/CChyv67YVL58ReVtYUeDh/YoU7rczBmLZs6akJAQv2zp+ucvng4a3H3WzMVQQqCsFirkVK9u42FDx3CbREVFrlu//OGj+ykpKVWq1ICT8vLy4aLevn29cNGMd+/fBARU7qlknkAjCDY5e/omUV8pNEM/3wnS1OuTlpY2asxAkUi0aOHqZUvWi0XiKVNHw2WFqFWrF+/bv6ttm067dh6tU7vBjFnjL17KuKNr1y27devayJ8nLFywCi7fr6sWXb9xBcJNTEzgd/uOzZ069hg7ZiqRa8q0GeP69R0GKWvVqrd4yewzZ09ymUR8CofbMHnSHIhKk6QtWTo7d0OrcuXqj58Ec8tw81xd3YIfBnGrIaEfoRhBgo8hH8aNH5qSmrJm9R9zZi19/frF6DEDldvzCxZOb9SoxexZS/3LVliwaMaHD++IBhiJJbh48azHjx6MGjVp65Z98LBdsXIB1HMIh1uzaPGsEsVL7dpxpH+/YXD316xbxm1y4ODuXX9thds6f97KQYNGXrh4GmomFwU3/djxg8WKlVyyeK2lhWV4eNjwEX3K+QdANe7UqefZcyehIHEpxWLxqjWLodUASlGqVFlwT0REhOdynIGBVZ88fSiVyqbOAn2JiJDN+Pnx43suFu5+5UrVVG6YnJw8fuJwkL8pk+cqPwqgeMPvjh2/z52z/N8TV4cNHXv4yN//HD8EgbCX0WMHwcN19KjJWzbvcbB3HDqsF5QriJJIJBMmjXB2doXLNWjAz7v3bIfClnOn6ipF/qH4Mj019q1AH5VGbxpCvYIbBg8lKEywCg//+w/uQj1MTU3999Sxrl16/9hK9hG55s1aP3x4f/ufm0BfYHXatAVJSYmF3WRqVzGg8smTR27eulq92v+4W1ilcvWfOnTj8oenYu0f6jdq2IwLT0xMgA25qM+fIzas/9PGWjZTfLu2nZcumxsXF2tnZ6/uUAMrVlm9Zonskx0Udf/+nbp1Gh06vBduvIe7Z3DwPTCOihcruXXbRjCCQFC4fMaNndalW6vLVy6AXUPkRQd2VK1qTViGagBP5rPn/u3da2A+rxWln28wUxQt0mwTuMVgKcDtgOWBA0bUqdPQzlZ2uY4fPwTP/FEjJ8Kyg4Njn16DFy+d3b1rX1ju+FN3uPU+PhmzcENhgDs+aODPRG7C2drajRg2josCMTIzN4eGCTyr4JaBnfLs2WMuCsrVj606cNcfvCFnzpwA1YDHg7rjrFypOjzhXr95CXcWKryfX3FrK2s4eLB0QLzA/q0UWC2n0sNdnjZ9bFJi4vp12xVWkjJgQHFFt17dRmfOnjh79iQYTcHBQTIja+l6OGaIGjJ41JWrF/fv3/XziPGX/jv36VPErys2c4cKIT91apYzW3WVguQbhtHbd4LEFNFk8D7cIaiQCxfPbNSweUCFSv7+FeCKQDhcZTBkqlSuoUgJsSdOHomFmm9rB4/tAwd2Q1NC8bQvXNhDkbJE8dLcAsMwr16/aNjw6z0YPGikYrlo0RKcpgBcsYYyZGen9lChDCUlJYGp6edXDJ5UfXsPefrs0cPgILmsBFUKlDnkHz26D49BhTa5uRV2d/d8EHyPkxWgWtWM2wy7LuJbNCw8hOQbVj/fYGZZRsOZMKGRu/fvHdCQrFA+EKz9kiVk9wtuFtj/PXsMUCSrWLEKBMLVA0EBk+TW7WvQEHj56jln/YHWKFJCS0qxDBZi8eKluI8QAk2btII/RSzskVuwt3OA39SUlFyOE6qxu/zmgqzAHQcD08LCAgwrUIEHD+5CE6ZIkaLQllGkp+SAFELBWL92OxRsldlCboplD3cvUBYit33gHDlN4bKCAg8SBsshIR+guQdliYuC/bq4uKrIN9dKkR/kTQ0+Hlwa+1YYKctoMuekmZkZtEvBDoSHDHhS4C727jmwUaPm0BaF2Jx9wNFRkVAhJ04eCa71Af2HQ1MTVrMlMzXLmKMbZALKpZmZucpdg0msWM7P1XR2doG2LhR9uK8gLlDo4VkHpaFJk5ZQ9OHxS2QugPinzx5Dj2O2Y1YsW1paKpbNLSzAPiL5h2L1sNOF0rwkThg/88iRfefO/wviAs//tm07gZqAWIC1D2UA/pQTgzELvxs3rQZbBpo/8KSB2r7597XHTxxWpFE2CsAgVVefidJNz+dRQz2HR0W7tp3APgULCMoSNC4gHO54xUwJUABPYM7WhjKprtQB5uYWSsvmcMBEXnLg9LOVHO5EoJBYWFgqh+fMHMp57pUiP8jcZLw8uHLxrWjNZevt7QsmH9yzu3dvgj0yf+F0H1+/Qk7OEDV2zBQPDy/lxGC7gt8LHKJLl6zjDAQivyXOTi45cwbNommau21aAfYI7hW42WCwgECUK1dx/YYV8NSF9jZ4ZyGBYyEneBRn6xrgTCEOUDrO0QjITFaNniesPva6sJqXRPCqdu/Wt1vXPtCW+e/y+T93/G5tbQPNHLikjRu1qC1v5ypwL+wJezh6bH+H9l1btmjLBXJPHZVYWVknZrZzv59Klar99tuvcIvBKgmsWBWMoNDQj7AKj5OunXur3PvM6YvAYQyGFbRoVIqX8sHLy4NMZeBZBabQvLkrlFOK5M1LaOIlJycphyflOMH8Vwp9AHwrRKPvBFEiQmsy/hbakyAlRC7bNWvWBrc5PE+eP3/i6eFtJjc6oE3E/YFz28e7CJQ8uKkQrrhk4CSHP5WZQyEoWbKMwrEKbNq8Zu265eRbAR/eg/t3oYuhQoVKsAp+QTh+aKKDMkJHI4QU9SsO/VZgaSsOG3xvEKvI4cWLp9wCtKfevXsDNjDRBCPw2CYmJh44uAeqE1Q5kOChQ0bDVXouvyzQLIVOIsWlg0YHeD3B4IfHODhBnTLvOLSOr167pC5/uONgXyjc5OC9GvfLUM7t+g3AYYRHhEEmRYsWh7IHZRLyhzsO972y3DeUDSgAAQGVZs1YDKVu564/VOYJbhrF8suXz/yKFOPOHc4RnpqK03d1LVxM3lxycy0sc/FktrZevnz+5cvnbHnmv1LkgnyiPz6eW7l8J0jNKFsp0agRBAYe9M6s37AS+lCgTQh3AgoElCe4hb17DQIfLedkgT4g6GHhRhaCvoD07Nn7Z1x8HNxdcKOC8y88QvV3uVq36gDucUh8L+j24SP7/tq9DdrD5FupGFAFdnTt2iU4QiJv0UA7GTopKmX2CHTo0A3MUei/gHIAp/PbxlV9+3cCnx8XC4cNLmQ4Zlnf+R/r4Bf6UIkG6GMHMzxIRJq4bEHroRNn5uwJYKpAl+qpU/+8ePkUBBqiBvQbfuXKBWjdwDWE+z57zqQx4wbD3Yc2DkgzPH5C5JYCOC8gfXx8HChUzvzB8QGbLF8x//adG2AKbdq8GixfkUhDr3Im4CaDzgRwnXJ3HIAFuONgroJ9oW4riIXGyNZtvz3PfIooA06iGzevwgL48qFYcr4/sDKqVq25dOkc6JyCczx0+O/BQ3qclD9xoTcdrsDS5XOhUIGgzJ47CeyXbHlqVCnUwfA1tSP4Vlq1aqUySq3LltLkwQA+2jGjJ8MNgGY2kfneq0Hnn6+v7JOg4K0ACd+1eys0jsC2LFum/Nixsj5jaFpDvx0UzdZt6kMTacqkOZFRX6ZNH9erT4d5c7JbIuD4iIuPhcRQBKEcQL8DdCqRb8Xa2hoeVmBtKlxrZcuWP3hor2IVzPvfN+/ZvXvboCHd4e6C+/aXcdO4Ti6pNN3S0gpMfehQB38BlLypU+aBx1qD3etlBzM8SDQyBcAsnT1zyeq1S7jGP6j84EGjuMFEYLxs3LATHi0gxykpyXDHoSOWM1qnTZkPHai9+3SAzYcOGQPug5s3r7Zt33Db1v3Z8odLCj2sUD9BhmDbJo1b9u8/nHwH4EOB6goNXm4V7jj4AaHvMvet4EbDEc6cOR7KQ7YoaD39/vvaiZN+hhZ6u3adFWNnFsxbeeToflCNx4+DwYsHcgOxRF7qoFt948ZVLX+sA6c/cMDPnJdXmVwqxbY/9hE9Q+NvMG+b8xaslQ6jfAmibe6c/fLwcuzw5d9ubemCa8cj756N6Tldv45KP+GGa+YcO6cnHFj1DjwYPab4EB2TyzeY1U6MgK+06gpKP0fZ4usaRoL+jlsxaE2B9vzkKaPUxe7481Aug+X4QC8rMFjylCFPktHqx7rqoiZMmFnrf3UJom00/06QzJdsqA8vWdt+4y51sQWsKTL0cZStoX/CJZc7Dr14RKuAQ+382dtEXwHvOz/2sMbjVjQdDqdvcGOf9RW9nDSWMuxJJ/X7jvMKeN9ZXr4kl8u4FfSt8I38cwt6pyv6Ob8uos98i2+FwmKmG1i9nDhWNoQKJyA1CuA+8uMm09i3Av47PZzG2VjQx54gmWvFkJu9iAKZAcHLrdR4vhVpOivFz4/pCn1sb1B62e2NfBv8WCuaz2UrAr3BUqYr9NFjK+ipbI0KmZuMF2tFc98KQ+HgKEHBoo/eWBCJeXKTaT6XLU68LzjwjhsJ4MFgeZEVjX0retn8R3SIfO4khleCAAAQAElEQVQfgiD5R/O5bPVxAjNEh6DDFtEUjX0rYlMRPrt0hFhMi02JvgHNcT08KuQbMDWnaVEBz2WruhFkbSuSSlBXdEJirNTU9BunI9IdTq7mOFLJOEhPYy2s+ChgGn+DObCuU3LCN07wh+RO2OtkFx8LomcUq2gB7aDX95IJYuCkJEoD6xQiuicX34pqWfEqbWrjaHJozQeCaJV7Z2IlKdKW/VyJ/lG6mt3N0+EEMWSOrPlo7WDiXcaM6J5c5rKlcpn849D60Oiw9HK1HUpWsSHI9/HlXdqNfz/Hx6QNmFeE6CuvgpPP7IwoHmBbpZmWJxNAdM2z23EPLkU7FjZrM9iNFDRU7nMKHd0YHvomCfwsjFSDgXvyTwbm2zUjeyM/X4nhSPPVX5G/DPOVWx5ZcW8j5707WkzRFG1XyKTLBM3m6OefW6fiHvwXmZrCyN6vz3W0Zp4j6PK6wrmP7M0tVp4xq/mGX6OyHfw3ZKju7NRlpf5yqchfXWJ1mdNiWiSm3YtYthrInyGs8Vy22UhOJmm5u1qozCvBfl3LHqtyjbt4rKoMc4bL6y+rKuWI4SOmTZvm4uKi2IGKe0VnfQUrP7vIuYnyiVDynnhFoFJstr2LRCJrQ3v8J3zOa85sSsXVI2ouiNqt1V203MWBZI1Vf+WzbXTx0qXg4OBhw4ZlT6d+syzlQTlZLntSLn7qKoXKbPK8dGrCTa1FFrz76zSeyzYbcMQWFnrXeaHMl7i3ds4i+COIlrA2xospFcUy4jgsJ1pB83eCDI309HTlr6YiiEqwnGgRjcetGBwSicTExIQgSK5gOdEiGo9bMTjwKYTkBywnWkTjd4IMDigu+BRC8gStFS1i/L4VqVRK41ysSF4wDIPlRFsYuW+Fkc/CisUFyRNsBGkRI/etYFlB8gkWFS1i5L4VLCtIPsGiokWM3LeCfjgkn2BR0SJG7lvBRxCST7CoaBH0rSCIDCwqWgR9KwgiA4uKFkHfCoLIwGGTWgR9KwgiA4uKFkHfCoLIwKKiRdC3giAysKhoESP3rWCDGckn4IZDWdEW6FtBEBlYVLQI+lYQRAYWFS2CvhUEkYFFRYsY/7gVLCtIfsAhTloEfSsIIgOLihYxct+Kl5fX69evFy1adPHixdTUVIIgavDz8zMz4+NDokYP2H2//PLL+/fvVcbm6/Nj+g8I5+XLl69fv37jxo1SpUpVq1atevXq5cqVIwiiRIcOHZYuXerr60uQb+XUqVNQvyiKevz4MdQylWmMRFaUuX///nU5r169qi4HroKHhwdBBE/nzp3nzp1brFgxgnwTU6ZMAcWYM2eOSJTbJ9yMUFYUJCUlcfYLAKucvsCvBf/flUT0g+7du0+dOhXsWYLkG6lUumnTJltb265du8bExNjb2+e5iTG7rywtLevLgeWPHz+CxBw/fnz69OklSpSoJqdChQoEERLgrwWvLUHyR0REhKur69mzZ+G6gaEHIfnRFGLc1oo6oJXEmTDPnz/n7BfA09OTIMZO//79hw8fHhAQQJC8AI8s2CnLly8nmiPEzrYKcgYOHJicnAziAlbMjh07QF65VhJgZWVFEGMErZU8+e+//8ClDb2rzZs3r1evHvkmhGitqCQkJISTGPiFbkhOYvCxZmSAqQLuFXX9F8iqVavevHkzf/787/Q/oqyoIDg4mNOXp0+fcvYLFERvb2+CGDijRo2CPuZatWoRRIlt27aB5T548ODIyMhChQqR7wZHHKqgnJwBAwakpqZy+rJ7924wnhWOGGwlGSjYCFImOjrawcHh5s2bcXFx4BOAEK1oCkFrJf+EhoYqWknQ+OQkpmLFigQxHCZOnNhQDhE8S5YsuXv37l9//UV0AMrKt/Do0SNuxB0sKEbc+fj4EES/mTp1KrSAmjZtSoTKrVu3bG1tS5Ysee7cOW7shS5AWfku0tLSFCPuUlJSFCPubGxsCKJ/zJo1KzAwsFWrVkSQ7Nq1Czp6Fi5caGdnR3QJyorWCA8P5yQGfsG/y/l6K1WqRBC9Yd68eWXKlGnbti0REtDSgY7OcePGffr0ycXFhegedNlqDTc3tzZyiLyVBPqycePGBw8eKBy9+IZbgSMol21iYiL0LTx//hzcgpxHlh9NIWit6BqJRMLZLwD04SlG3OnaCkVUsmzZMnd39y5duhBjZ8uWLdBtfPHiRVIQoLWiW0xMTGrJIfI3LEBiLly4sGjRIijcnMRUrlyZIHxh9NYKWMfwJIOmN5jGBaUpBK2VguLJkyecIyYoKEgx4s7Pz48gumTt2rWWlpZ9+vQhxsipU6d27969YMECV1dXUqCgrBQwUqlU4ehNSEhQOGKwlaRFmjVrRuQN0qSkJIZhuGXoZz1//jwxfA4dOgRGyvTp03nzyOYJNoIKGJFI9D85sAzFAvTl0qVLS5YsAQcwJzFVqlQhyPdRvHhx6FjNNvOQoQ/hB31MTU0Fh93Dhw8HDRpEePTI5glaK3rKs2fPOEfvnTt3FCPuihYtShDNgYo3evTo6OhoRYizszNot7+/PzFMDh48CI0d8NNZWFhQFEX0DJQVfQceSooRdzExMYoRdw4ODgTJN5MmTTp9+rRitW7dukuXLiWGxosXL8CkBdsWmm/fPGsBD6CsGBJfvnxROGLA4uX0pWrVqgTJC6iQI0eOhDoJy+C3WrhwocG1Lm/fvg0d5GCk6P8AKJQVQwVaSZwJc/PmTYWjFyd/zoU5c+YcPnwYFqBTf8OGDcRAOHnyJLjb5s+fDw8VJycnYgigrBg8cAcVI+7AfaAYcZf7S+4DBgzYtGkTERIhISFDhgxJTEycNWuWQfhrufmop06dCjfLsF5kRVkxKiIjIxWzNzg6OiocMdmStWrV6uPHj+XKldu+fXu2qGObw8PeJEvSGCadUbEDloIioyKYgNswezjLECrj83YUyRKbbTUDhqXofGeeexQr34dGm8gPgNBqNlN3bJmZqr4sGZHyXeZCPs5C9RVTQItoWkzZFTLt8oteTMmMsmK0gDdB4esF/wunLyVKlICoOnXqwEMbnMHe3t6///67wq7ZMf99ejopFmDj629PkRyyQslrSM49qSzzqgIzwuTVBaohm5kXS8n+yykGXHqIoZmviZUzV84kyz5z6ooiLkdUxt7ViVHWE8m2R3V7U942F43LSy4y05A8ktEi0af3iU9uxsV8Shm8qAgpaFBWBAEnLqAy0D4HfTl+/DjXKwl3v3DhwsuXLwe52bnwA0WJWg12J4jBEvJcemH/+8ELC1hZUFaERVRUFOjLlClTaPrr57ddXV17tVwc99Gq03hfghg4xzeHgh1asK0hY/i0O5J/wOGycuVKZU0h8pli3jyMs3fDb54bA2Wq2sVGppECBQfvC474+HgiH2UH7SAHBwdra2sLGbaWVlgYjAFXPyvV7nYewZIkOEBHoNUDztqyZcsWL168SJEiTk5OGya8Sk2TEMTwkUikUikpWFBWBMepU6cIgugSlBUkA717Xw0xWFBWkAywRxDRFigrCIJoGZQVBEG0DMoKkgH6VhBtgbKCZIC+FURboKwgCKJlUFYQxLjQg9YsygqCGBd60JpFWUFk0DT8odMW0Q4oK4gMhqEYBp22iHZAWUE4UFMQrYHzrSBGxazZE4+fOEyQAgVlBTEqnj17TJCCBmUFkUFp7rJ9/Dh44KBuzVv+MGHSz48ePRgxst+KlQu4qKioyLnzpnTu2rJNu4bzFkz78OEdF37w0N52HRq/f/+2T7+O9RpU7jeg88l/jyoyhEzGTxj+Y+t6PXq1W7d+RWJiIhe+/8Du9j81uXzlQoNGVVevlX2K8Nq1/+bNn9qpS4tmLWqNGTv4XtBtLiXkGRYeumTpnFat68Jqenr6bxtXwb5atKoNB3n9+uX8nNebN69+XbWoV58OTZrVHDS4++Ej+xThkP+Tp4+mTR8HCx07N1+/YaViapPrN66MHjMIjqdbjzYLFs2IjPwCpwnJ7t+/yyU4c/YkrMIV4Fa52MdPHsIyXIShw3vDtvC7b/8uxTywM2aOnz1nEpwCpLx95wYxHFBWEBksQzRy2aakpEyeOtrBwXHL5r39+g5du375588R3LTbUNNGjx0UdP/O6FGTt2ze42DvOHRYr5DQjxBlYmKSkBC/avXiX8ZOO3fmVp3aDRcvmR0REQ5RH0M+jBs/NCU1Zc3qP+bMWvr69YvRYwaCLkCUqalpUlLikSP7Jk2c3bZ1R9j1vAVTU1NTJ06YNX/eSm9v3ylTR4OQQcqTx6/A7y/jph09fAEWYEdQS9u26bRr59E6tRvMmDX+4qWzeZ7a2nXLbt26NvLnCQsXrGrevA1IDEgGd/Dwu2z53AYNmp46eW3KpLl7/95x/oLsA6zPXzydNHlkxYpVtm7Z9/OI8a9ePV+0eCYcmIuL66PHD7hsHz4McnV1e5y5GvwwyNrKulTJMiA3ixbPKlG81K4dR/r3GwYHvGbdMi4N7PH1m5fwN2/O8hIlShPDAWUFkaHp18Gv37gcGxszaOBIN7fCUCUG9B/OqQMQHBwEj+LJk+ZUq1rT0bHQkMGjbO3s9+/fxcVKJJJePQeWKVMONKhJ45bwZH758hmEnzlzwkRsAoICtdHX12/c2GkvXj4DC0V+bBRISefOvRo2aOrp6W1ubr554+6xY6ZUDKgMf4MHjUpOToZamu0IQXf+PXWsa5feP7Zqb2dr17xZ6wb1m27/M+8vrk2btmDJknWBFatA5q1/7FCyROmbt64qYkEK69ZpCBW+QoVA98Iez58/gcCHwUFwVN279QXhgLNetmR9ly69IbxiQJUncnsEuP/gbtMmreBXcZUqV65O0/Tx44fKl684auRE0GjYaZ9egw8d2hsdHcWdeHh46KwZi2vWrG1rY0vyCVvw/neUFUSGpt9fePPmpbW1tZ9fxrdZoQbaZJZ7qOFQ66CGcKtQNwIqVFJUJ6BUqbLcArcJ2C9E1gK6D+F2dvZcFKiVu7vng+B7X7cqWVaxDMbL6jVLOnRsCq0DaDsQ2QcAo7MdIVT4tLS0KpVrKELgMF6/fhkbF0tyh2UPHNjds3d7yBz+nj57HCOv5BzKVoO1tQ138P7lAkD4Jk0Z9fe+nWB2wVnABYFwuAjcKYAEv337+sdWHaBxxOkvXKXAwKoMwzx8dF/5IMHkgUDFift4FwHBIhpBF/xAW+xgRr6F+IR4S0sr5RB7ewduAWoamCRQIVXGErnQ5MwQtoIKnG2raHnThgOaQtwCVMuRo/sHVqw6bcp8zupp1KS6ygzhFzw+2cIhTzBeiBqgSk+cPFIiSQP7KwC00tomWw7ZPlrAAfYatJguXTq7cdNq8ApVCqzau9cgf/8KlSpVi4uLBdsNGjLFi5UE2w0O+MGDu1Wr1gwN/Vi1Sk0QPrhWv29ZB39ZDjJTyEzNNP8cAo6yRfQEqOm0Jg0hczNzqBLKIZGRn7mFQoWcLCws5s1doRwrokW5noMXTgAAEABJREFUZ+hYyKlcuYA+vQcrB9rZ2udMeeHiadg1OFZgL0SVnZJxGE7O8AttJQ8PL+VwFxc3oh7wkjx9+mjpknUgDVwIyJOzkwvJC2j7wB8c/507N/Yf+GvylFEH9p+GS1GkSFFwr7x89bxc+YqQrHy5irBKi0TQgIIWE4RYWlo2btSidu0Gyrm5F9aLb55+MygriAxoBDGaNISgrkJ9BkcpPIFhFfpikpKSuKiiRUuAswNqr4d7Rt0IDQuxt3PIPcOifsVPnf6nQvlAhTkArQbwpORMCc9/aD1xmgKo88J6enibyR/1XHuEyE0AcOVANSbqgdYK/Cp0BI4B/or4FiW5EhR0JzUtFWTFycm5SZOWbm7uo8YMDI8I8/TwgkYNdAaBB7p7d5nVU84/YOPm1eCKBsdKxokXLQGmn+IgwXgJCwsBXy8xZNC3gsig5J9Fzn/66tVqiUQicHBANzB4E/78c7Ozc0ZVhOc8GPlLl86B1grU0kOH/x48pMfJk0dyz7BDh27QAIFOEHBSQIc09Kr27d8J2g45U/r5FQcPxZGj+6Fy3rh59e7dm+DL+PRJ5rAAHYHDuH37OsgcNJqgJQI+WnCOgnUD6gM9TSt/XZj7Yfj6+InF4j17/4yLj4PGC5xglcrVQSBy3wr8IzNnjT967ABILfQZHzi4G/TFzbUwRAUGgKzckVkr/gGw6u8f8O7dG7BoAjOtoQH9hl+5cuH4icNw+nCo0KM8ZtzgbJagwYHWCiKDzfgGeX4B8370qEngEWj/U+PixUtB5w7UQLHYhItdMG8lVPvZcyc9fhzs5eXTsGGzdu06554h9HT8vnnP7t3bBg3pDvUZ3LfQTww+i5wpG9Rv8u7da9CLFSsXQJ2fMH7m7j3bd/21NT4+bszoyd269v1j6wbou/lr17HOnXqCLbBr91aQHisr67Jlyo8dOzX3w4CGyZTJc7dt39i6TX2wyKZMmhMZ9WXa9HG9+nSAXl51W3X8qTsIypq1S5evmA9yVr9ekxXLN4I8QRTIB6gSdG9BRw+Rf6QJ+rnAc1wx06UNTb+NG3bu3PUHKGlKSjIc5Nw5y83MDPsLk/gNZkTGhgmvXItYNOyiwXfdQ0I/QmOE6/iEUtTyxzp9ew9p374LQQqUuGjpgV/fjFhRjBQcaK0g3wK0boYO61WsaIl+/YbBc/j339fSFF23biOCICgrCIdIRIk1GbwP7oyF83/dtHnN9Bnj0lJTS5f2X7tmK7SMiN4D/gvoplEXu+PPQ4qxM8g3g7KCyJBK2XQN51sBKVm+bAMxNMCXsWvXUXWxNtY2BPluUFYQwYHaoWtQVhAE0TIoK4gMiqJwJltEW6CsIDJkfhUWhQXRDigriAyKsCyFI5gQ7YCygiCIlkFZQTjQt4JoDZQVhANf4kC0BsoKgiBaBmUFkUGLKbEYm0HGAM1qNiOXLkBZQWSYmYnZdJx8xxhISpSKTApYVrAkITKc3U0jw1MIYvg8vhZtblnA9RplBZHRvL9bWgrz/A4qi8ET8iKpdrsCnrMSp3FCMpGS9ZNe+5Sx+aGtM0EMkKfX4+9e+NJmkIerrykpUFBWkCxsmfkuNUkqEpG0VCb3lBT1tVNavkjlTEDUd1xzm1O07IOKajOnsnyeQrHHjG0pFZlnSyNfYrn3Er5G5dhpzq2UNpcdg7pNch4DTVMMw+Y/fZarpHS+6s6OqLqkJqZQkSkRTVVqVCiwfr4/VKYzUFaQ7MSEk2f3o9OSJfnfBKoRlUMeWO6LQHnoCqU6QUYFU6MrWRMR1UGZWVCZhZyiQkJCoiKjypUvp6JyZ5MTUCJKaUGlHuRUPplmwZXQIL2yVLBZPhym4uzU6YrIlHb1sipazoLoB9gThGTH3o1Uc3Mgxsjhw1cjIx/80Lo+QXQJWiuIgIiMjExOTvb0NOyPe+k/KCsIgmgZ7GBGBMTp06f37t1LEB2DvhVEQISGhsbFxRFEx2AjCBEQERERUODd3NwIoktQVhAE0TLoW0EExP79+0+ePEkQHYOyggiI9+/fQx8zQXQMNoIQARESEmJmZubkZAAfdTVoUFYQBNEy2AhCBMTWrVsvX75MEB2DsoIIiDdv3sTGxhJEx2AjCBEQ7969s7W1dXAwzhcp9QeUFQRBtAw2ghABsWbNmvv37xNEx6CsIALi+fPniYmJBNEx2AhCBMSrV69cXV2tra0JoktQVhAE0TLYCEIExMKFC8FgIYiOwflWEAHx+PHj1NRUgugYbAQhAuLZs2c+Pj7m5uYE0SUoKwiCaBn0rSACYvny5Z8+fSKIjkFZQQTE1atXk5KSCKJjsBGECAj0rfADygqCIFoGG0GIgJg3b97bt28JomNw3AoiIHC+FX7ARhAiIPCdIH5AWUEQRMugbwURECtWrHjw4AFBdAz6VhAB8fHjx+joaILoGGwEIQLi/fv3dnIIoktQVhAE0TLoW0EExKZNm65evUoQHYOyggiI8PDwz58/E0THYCMIERChoaHm5uaOjo4E0SUoKwiCaBlsBCECYvfu3adOnSKIjsFxK4iAAMcKzmXLA9gIQoyfli1bMgwDC/BLUZRYLGbl/PPPPwTRAWitIMZP4cKF79y5Q9Nfm/ygLzVq1CCIbkDfCmL89O3b18nJSTnE2tq6S5cuBNENKCuI8QOGSdmyZZVD/Pz8atWqRRDdgLKCCIIePXo4ODhwy1ZWVt26dSOIzkBZQQRBoBxu2dvbu1GjRgTRGSgriFDo3bu3m5ubqalpp06dCKJLsIMZyS9XjkS9fJCQlixNTZGqTEBBcSKsqnACwSylOltKKYqiCFceFQvqyJkgz21pipJKGdgb1yWknIwihFV9Rl/D8zwkyFXei531YGT/Urnlq4acF9PUhBab0S7eFi37uRL9BmUFyRdHfgv79D7FydPS0c00LU2iOpGamkdRcmFRU9IgVhEFNZ+RL3OBNKEYNfVPhYRl7p2mKYZRsZUIZEXpGJT3q7ycJUtW9l/2Y1MKzJI/TUszdeXrlaBowjJE/SlzJ6IizxwX08TMJCk2/dPbpDQJM2BuEaLHoKwgebNj/of0dLb9SG+C6AE3j8e8vB81aKEf0VfQt4LkwZUjkcmJ6agp+kPV5vZ2zmZ/Lf5I9BWUFSQPXt5PdPWyIog+Ua2Rc+yXNKKvoKwgeZCaLLV3MSGIPuHkY8oybEIU0U9QVpA8kKQwjFRKED0DerWk+npf8FVDBEG0DMoKgiBaBmUFyQuKZWiKIEi+QVlB8oKlaAYHN+kdssG7+qr2KCsIYpBQhCX6qvYoKwiCaBmUFSQf0NgIQjQAZQXJBwy6bPUOJuMVTn0EZQVBDBKa6O9rwigrCGKQyOdoIPoJygqSBxTFUiJ8yUPv4CbH0k9QVpA8YFmKlTIEQfINPoUQY2P/gd0NGlUl/ML/TkHu9bYRhLKC5AXFEr3tclBFmdL+Pbr3zzPZwUN7FyyaQbREPneqRSgKh8Mhhgub19zQekbp0v7wl2eyZ88eE+2Rz50KBLRWkDyQWSoaFpPrN66MHjOoWYta3Xq0AYsgMvILF37t2n/z5k/t1KUFRI0ZO/he0G0ITExMbNSk+o6dWxSbS6XSFq1qb9y0GpajoiLnzpvSuWvLNu0azlsw7cOHd3nuXbk9AlsdPrJv+5+bIaTlj3VmzZ7IHcyoMQP/PXXs1Kl/6jWo/PzFUwg5+e/RocN7w4HB7779uxSdtzNmjp89Z9JvG1dByi1/bIDfhw/vK/b15OkjCIHzVd5peno6pO/TryOcxYRJP1+/fpkLb9eh8bbtm7jl2NgY2BCOR5FVh45Nz184TYwClBUkDxiicp55tUAtnTR5ZMWKVbZu2ffziPGvXj1ftHgmhKekpMxbMDU1NXXihFnz56309vadMnU0qIaVlVWN6j/89985RQ6379xISkpqUL8p6MvosYOC7t8ZPWryls17HOwdhw7rFRKqwRyuJiYme/Zsp2n60MGz2/7YH/wwaOu23yB85fKNYFw0btzi/NnbJYqXOnP25KLFs2Bh144j/fsNA1lZs26ZIofXb17C37w5y1v/2MHG2uaS0qFevnweQqpUrq6801WrF0MObdt02rXzaJ3aDWbMGn/x0lkIr1y5+uMnwVyau/duubq6wfFwq3BSoHclSpQmGoC+FcRgkX1pQhNZeRgcZG5u3r1bX6g21arWXLZkfZcuvSEcAjdv3D12zJSKAZXhb/CgUcnJyVy9qlOnIYhRWHgolwPUVV9fv6JFiwcHB71//3bypDmQj6NjoSGDR9na2e/fv4togoeHFxwMVP5ChZyqVK7x/PmTnGmOHz9UvnzFUSMnOjg4Blas0qfX4EOH9kZHy+Z0pCgqPDx01ozFNWvWhhzq1Wt86b+zig1BYho0aCoSiRQhoJtgB3Xt0vvHVu3tbO2aN2sN+rj9T5mRAjk/fBjE2UH379+pW6dRQkI8p5LBwffs7R083D2JBuhvyxRlBckDWSNIk6eif7kAMEwmTRn1976dH0M+2NnZg4hwUUlJiavXLAFrH+x/aG5ASExMNPz+r2YdMzMzzmCBWgfPdqiKsAyiA8YC1MbMI6ECKlS6/+Au0QRlE8DGxjYxMSFbAoZhHj66D4qjCAFTCwIfBN/jVn28i4Amcst16zaKiAjn2k1v3rz6+PE9d6gKQLbS0tKUc4Njfv36ZWxcbKXAamCFwVbcqZXzDyhVqiyosGw1OKhSoOYdSeiyRQwUTYeIQ1Ni4YJVly6dBefIuvUroLb07jXI378C1MaRo/sHVqw6bcr8MmXKgUaAS4XbBCptzRq1/7t8vuNP3aGCxcfHNWrYHMLhYS6RSECDlPOHpzrRhDzfnAEVgL38vmUd/CmHc9YKYGpmpggEjQCLBs4OThMO2NnZBU5NeSs4ZvgdMbJftr1ER0WCCebl5QMSBlYPiAuI15OnD0FfmjRpCRLWuVNPoiF62z+HsoLkBUVp+k4btFngr0/vwXfu3Nh/4K/JU0Yd2H/6wsXTUIHBsWJhYUEy7RQFYAWAcxT8C9CsKFu2PDSgIBCqHySeN3eFckoRLSJaBUTN0tKycaMWtWs3UA53L6yiSQKXAtpBl69cABcMNNY4+VOmkJMz/EJbDxpfyuEuLrIzApEF9wooo59fMdhpuXIV129YAe5bsHrAwUQ0RG9bQSgrSF6wYLBoMMo2KOhOaloqyIqTkzM8h93c3KHbJTwiLC4uFtognKYAnBdTAVQq8N1ev3H53Pl/FQNAihYtAf4XqJAKp0NoWIi9nWbWSn6AHcUnxCsaa2C8hIWFuLio/tRx/bqNDxzYDf07L14+A79PtlhPD28zuXWjyA2sHrD4QERgOTCw6vr1K6ytbCpUqASr0A4C59GZMyfAgQ3OI6IJ+vxOEPpWkHzAalB+wcifOWv80WMHwB55/OThgYO7QV/cXAv7+RUHY+TI0f3Q/3rj5tW7d2+C2+XTp3BuK/Ch1KxZ58iRffDoriekqm0AABAASURBVFunIRcIz/aqVWsuXToHGlAQfujw34OH9Dh58gjRBmBNPHnyEHpkoNoP6Df8ypULx08cBpcKtMKgR3nMuMFgW6ncEIwpUJw/tm4AiwPaNdliQT6g0Qc+WsgHcgD1HDd+6MpfF3KxFQOqgMJeu3bJv2wFLnHxYiXhElWqVI1oiD6/E4SygmgZ8I+0aN52zdqlbds3Gj1moKWl1YrlG8VicYP6TXp07wf1DVwq0JsDfc/Qgtj119blK+ZzG9atLesPAikB54UitwXzVkI/0ey5k9q0awjVr2HDZu3adSbaoFWLdtCi+WX8sFevX5QrF7Bxw84HD+7BMYMKgFt37pzlZkoulWxAJw4cav16TVTGgpfkl3HTd+3e2qp13V9XLYLG1NixU7koa2vrkiXLgM2l8EODSCmvGgf4aXckD9aOeVW2hn2lxpqZ6Iiu2TbzRY/JfnbOWvY0aQX0rSD5AZ89iAagrCB5QNEsJdIv3+CkKaO44R45ad68zZDBo4gg0N8XQFFWkDxgGYqV6pe1Ar3U6RKJyigzM3MiFPTXhERZQfKCks+NoE/Y2doRRLMOOl5BWUHyQO7Ux5n39RIcvI8YKJT8+3kIkn9QVpC8kM23gtYKogEoK0g+QGtFH8GeIMRwofTWMyhw9HckK8oKkheoKoiGoKwgecHq8Vc5Eb0EZQXJA5Yw6LJFNAJlBckDiqIplBV9RH9vCsoKkgemFiJpGsqK3kGLaWs7fXx9meB8K0ieWNuJP31IJog+8ehavImYEpkS/QRlBcmDZj09oyNSCKJPBF+O9C1jRfQVnMYJyZuQl8lHN4U17O7p6q2vz0chse/Xd55FLRp1cyH6CsoKki9unYy9c/4LbSIyM6fSkvMoM5TsLSJWMeCFyvER54wQilU5KOZrelUJlHLLeAdSKb3ygOCsb0h+jVIdLt8TlSMxuDBYJj3HQWYemNKuGcLSWTdX2hEXQuW4JrTsq5E5s+U2kSVROn2xKWGlJDWZcfYw7zDKnegxKCuIBlw5GPMpNCk1SZJ7Mq7niGVYxapiOUsIJGNUFD9KREH9kY3Dy7Ghcm4UlVF6v4Yop6dpwjDZtkoBJGn2NrY5c1PWEoomim8NiExoqSTHhwcyj/xrSuVz4XJSCuH2Ij+vLLugRER+pplp4J+vxw8ZUIzSTDcm5mJbB5P6rVxEej8tBMoKIiCOHDkSFBQ0ffp0gugS7GBGBER6erpYjGVe5+AlRgQEyIqJiQlBdAzKCiIg0FrhB7zEiIBAWeEHvMSIgEBZ4Qe8xIiAAFkRifT0PRpjAgfvIwJCIpGgy5YH0FpBBAQ2gvgBLzEiIFBW+AEvMSIgUFb4AS8xIiDQt8IPKCuIgEBrhR/wEiMCAmWFH/ASIwICZYUf8BIjAgJlhR/wEiMCAl22/ICygggItFb4AS8xIiBQVvgBLzEiIKRSKcoKD+AlRgQE+FZQVngALzEiILARxA94iREBgbLCD3iJEQFhamqKHcw8gLKCCIikpCT8MBYPoKwgAgJaQNAOIoiOQVlBBATKCj+grCACAmWFH1BWEAGBssIPKCuIgEBZ4QeUFURAoKzwA8oKIiBQVvgBZQURECgr/ICygggIlBV+QFlBBATKCj+grCACAmWFH1BWEAGBssIPKCuIgEBZ4QeUFURAoKzwA8oKIiBQVvgBZQURECgr/EDhrDaI0dO+fXsin3EyLi6OoigrKyuGYaDkHz9+nCA6AK0VxPgxMzN7/vy5YhXEBWTF39+fILqBJghi7HTt2tXCwkI5xMbGplOnTgTRDSgriPHTsmXLokWLKoe4ubm1aNGCILoBZQURBGCw2NnZccvQJurQoQNBdAbKCiIIGjdu7Ofnxy17eHi0adOGIDoDZQURCr179waDhabp5s2b49eCdAp2MCPfxfE/IiLDU1MTGeVAsSmbnkYpVinCUDTNKCURiaDcUYoQlmJpSpaeVaShWCp7CENRtHIIS1gRTTFZ9kwoKmOXrHI4zcoPAfqAEhhGamNjS8lgZemypYQgWhYC+WSrGZSIsFKSEwoyZ+WHyqqOUlfDaDHDpKt9rptZUJa2prVaObr6mhFDA2UF+UY+vkw+tjnMzEJkbWealpKmHCUyE0lTv1ZBWRWmCSP9WtJokayysQyrSEHLAijlEK7aZwsBGWAyQ2TKIyKK1YxAmtMVVlksZFuCjNFKuSlSwmGw2XOQJYPYrDnDMSufgnLmckmhcuqKLIoiLKO6iolMKKlEbe0zszRJTpQmRkv8yls37u5CDAqUFeRbeHoz8eL+iB+H+lnbE0Sn7Fny1t3PvHlfN2I4oG8F+RbO749o3t8HNYUHOv3iG/Ii+c7pOGI4oKwgGnNy2yczC9reRUQQXnDxsXx0PYYYDigriMZEfUq1tMGeFP5w97FKSjSkNyTxnSBEY5ITpCZmDEF4QyRNTzOkC46ygiCIlkFZQRBEy6CsIAiiZdBli2gOlTGuFEFUgtYKojmywa44ihJRC8oKgiBaBmUF0Rha9qYLgqgFZQXRGAZcKwRB1IKygmiM7L1cNFcQ9aCsIBojm9MAzRVEPSgriMaIxEREo7mCqAVlBdEYaTqhRWiuIGrB4XCIPvL69ct6DSo/eHCP6Det2zbY/udmgmQFZQXRGJGIokW6bQTZ2zv07NHfxSVjSrS27RuFhoUQ/UD5YDp17FG+XEWCZAUbQYjGSKUsLSU6xdGxUJ/eg7nl8PCwmJhooh9kO5iuXXoTJAeimTNnEgTRhHsXYkRiqnTV/E452a5D45SUlIAKlWA5NjamWYta7969rlunIRfboWNTqVT6/PnTadPHenh49enXMS4+1tnJBbYKrFgF7IKBg7pBsgMHdr989ax+vSZRUZHLls9bt2HFjp1bXr1+UcS3qJ1d3kdy/cYV2OrXVYtOnDzy/MWTMqXLWVpaQnguub1//3b6zF8WL5l9+vQ/nz9/Kl8+8EHwvWwHA40giURSoUIgl37W7Imr1y7Zs3fH9RuX3dzcC7u5Q/jBQ3unTB1do/oPo8YMhAP47/I5MzOzYsVKknzzJSQ15GVS1SaOxEDARhCiMbKeIE0aQZUrV3/8JJhbvnvvlqurW/DDIG41JPRjZOQXSGBqapqUlHjkyL5JE2e3bd1RsW3FgMoL5q2EhZ07Ds+dvQwEaPTYQUH374weNXnL5j0O9o5Dh/WCTHI/gOcvnk6aPLJixSpbt+z7ecT4V6+eL1o8k8jMLrW5gVUyfESfcv4By5au79Sp59lzJ1etXpztYJR3ER0dBemh1bbxt11rV/8BWc2ZOzkpKQmiTExMEhLiYfNfxk47d+ZWndoNQaoiIsKJ8YKygmgMI5W1g/KfHoyOhw+DuG883L9/p26dRlDNuNobHHwP3CjFi5WkKAosms6dezVs0NTT01tdVsHBQWAUTJ40p1rVmtBQGjJ4lK2d/f79u3I/gIfBQebm5t279QVFgw2XLVnfRd54ySW3fft3mZmbQ0MMDv7HVu379R2a+xfL/t6309TMbNzYqe6FPeD4fxk3PTk56fCRv7lYsGh69RxYpkw5OM0mjVvCpXj58hnJPxQxrNclUFYQjdF0LFylwGrw3H7z5hUsg50CJkCpUmWhqhN5xa4UWFWRslTJsrlnBZtD9Yaqzq1CLYW21f0Hd3Pfyr9cAGjWpCmjoPJ/DPkAzRywO3LP7fXrF8WLlxKJMqYBb9qk1cifJ+Syi9dvXkJ6sTjDWWllZeXl6fP8+ZOvp1Yq49RsbGzhF4SV5BuGzfgYm6GALltEY0Q0oTR5Hjk7u3h5+Tx8dL9QIScQF2iMPHn6EKp0kyYtwVvRuVNPRUpoCuWeFdRGePJD37NyINg7uW9VoniphQtWXbp0duOm1evWrwAh691rkL9/hVxyS0xMyDNbZaIiv4BjSDnE3MIiKTlJsfo9ukATSt03zPQTlBVEY6QMoTWcsBlqMrhXoKL6+RUDX2m5chXXb1gB7tuPH9+DLzP/+YAwWVhYzJu7QjlQROf9aRFo5sAfNGru3Lmx/8Bfk6eMOrD/dC65WVlZJyYlknxjaWWVkpqiHJKclOTp4U0ECcoKojEya0XDZ29gYNX161dYW9lUkPcHQTsInBpnzpzw9vYFp0b+8ylatERycjJ4Rj3cPbkQ6Cqyt8vDrAgKupOalgqy4uTkDCYS9NFAp0x4RFguuZUsWebosf3p6elcu+bsuX9PnDi8aOFqdbsoWaLMv6eOge3DuWDi4uPevX/TuHELIkjQt4JoDFgrmn5jt2JAFajG165d8i9bAVbBYAE37YGDuytVqpbntl7evvB74cLpx08egtVTtWrNpUvnQE8KGDuHDv89eEiPkyeP5J4DtL9mzhp/9NiBmJhoyAT2C/ri5lo4l9xaNG+Tlpa2fMX823du/Hf5/KbNqws5OYOrRflglHfRqlV7aDdBXzVk9fbt6wULp5ubmTdv1oYIErRWED6wtraG5//Tp48U/tGyZcsfPLRXsZoLYEqAx/SPrRtAklYs/w26eI8c3T977qTHj4PBZdOwYbN27TrnnkPHn7qDoKxZuxRkAtw39es1WbF8I2eGqMsNenPAHQOKc+LkETMzM+i+6d9/eM6DUezC08NrxvSFf/65uXPXluASLl3a/9eVm8FxSwQJftod0Zg/ZrwTmZG2w3wIwgtPb8TeOPl5+PJixEDARhCiMQxODofkCjaCEI2RzWWrZ8+jXX9t/euvrSqjfHz91qzaQhAeQVlBNEY2O5yefREYPKb16jVWGSUWGX4hN7RRtigriMaAqaLriRE0xcbaBv6IscISw2p3oqwgGsMwhJGiewVRC7psEY2hZI9OnMsWUQtaK4jGyBpB+LFURD0oK4jGMAxlUC++IXyDsoJojKbTOCFCA2UF0Rj8oAeSOygriMbI3l5m0VpB1IKygmiMSEREJgRB1IGygmhMejqhsBGEqAdlBdEYCwtKZGJKEL5giMjU1JCGmOFwOERjHFwtkuLTCMIX4W8SzSxRVhCjplkfl9REaUIUtoN4IuJdUplq+f3Ymz6AsoJ8C7XbuRxe/yYtmSC6Zu/Sd+5+5lWaGJKs4OxwyDfy8Vnysd/DLGzE1nam6ZJ05SjogVYuVhmrFKvols6WIANa5kXIkg8tm4GBomWfs+B+FYHZUAQq56x6L7IIro8850GqeNeJFhMmPVsQSxgqy46445SHyH6pLCeiHCtbFRFW+nWnJMt3l74egIm5ODkuPT4m1a+sTeOeLsSgQFlBvoujG8OjP6WlJGaVlaw1P6NeyXWFC6Fpisk5/p/K/vp/pqDIc8uMVehLlpSZdTV7bScyCVAkY2CvLCsWyz7ZkbPyQ1DO6iA2odIlWQM5WVE+R6WQnDv9Gi5PT4tkn4XM3FB+Wpn5wP6pTOU1sxBZ2plWb2rvVdKCGBooK4iAOHr06N27d2fMmEEQXYIdzIiAUHz3B9EpeIkRAaH4PBiiU1BWEAGB1grewaExAAAJIElEQVQ/4CVGBATKCj/gJUYEBMoKP+AlRgQEygo/4ChbRECgrPADXmJEQKCs8ANeYkRAoKzwA15iRECgrPADXmJEQOBwOH5AWUEEBFor/ICXGBEQKCv8gJcYERAoK/yAlxgRECgr/ICXGBEQ6LLlB5QVRECgtcIPeIkRAYGywg94iREBgbLCD3iJEQEBsoK+FR5AWUEEBFor/ICXGBEQKCv8gJcYERAoK/yAlxgREDY2NiKRiCA6BmUFERCJiYlpaWkE0TEoK4iAgG4giURCEB2DsoIICHCsgHuFIDoGZQURECgr/ICygggIlBV+QFlBBATIilQqJYiOQVlBBARaK/yAsoIICJQVfkBZQQQEygo/oKwgAgJlhR9QVhABgbLCDygriIBAWeEHlBVEQKCs8APKCiIgUFb4AWUFERAoK/yAsoIICJQVfkBZQQQEygo/UCzLEgQxalq1ahUSEgJFnaIomqZhgWEYHx+fQ4cOEUQH0ARBjJ327dubmJiIRCLQFFgFcTE1NYVAgugGlBXE+OnWrZuXl5dyCJgqbdu2JYhuQFlBjB8wVTp16mRmZsatgtnSuHFja2trgugGlBVEEPz0008Kg8Xb2xtNFZ2CsoIIhZ49e1pZWYFj5YcffihUqBBBdAb2BCH6SGRo2st7iVGfJKkp6elpDIRQhLAyb6ssVlFmKZqwjOJXFs/K0hLwzDLyBUjPJYY0hCHPn79ITU8uUbSUqZmpLJmIYqRctDx38nVbLk/l/UI+ikAOEzOR2ISycRD7lLT0KWtJkExQVhA94v6luODLMfHREqjYNE1BtZd3Bn9NoFT9lVbl/8B/NJTnjAiKZCvYmVuy8kV1qRSBMhGhSO6IRBSY+0w6K9cm1sxSXKSsVYPOzkTwoKwgesHVo5HBV2KlUtbMxqyQh429u4H5U5Oj0yLexCTFJIHEeBSzaju0MBEwKCtIwbNlxtuUZMbR09atuAMxcOLDk0OffZFKmabd3PwCBNoyQllBCpIPz1KObgqxdrb0Lu9CjIgv7xIiXkb6lbVs1seNCA+UFaTASEsgm2a8Klrdy9zaOD+3/uzSh0oNHCo3tCMCA2UFKRie3Uo8sye8bANfYtQ8ufDB2d20w0h3IiRw3ApSAKSlkTO7w4xeU4DSdb2+hKWe3fOFCAmUFaQA+GP6Gycfg/fO5pNSdbyf3YxNiCfCAWUF4ZsDa0IJTbsWtyeCwcbV+q9Fb4hgQFlB+CbsTbJfoLCGdXiVc5KksTdORBNhgLKC8MrBtaFiU5GJlXF2/eSCrbPVgysxRBigrCC8Ev42xclbfztc9x9dvGR1F6IDPP2d0pKlYW9SiQBAWUH4401wEsOyhXxtiSARm4qvH48kAgBlBeGP4KuxYhPBNX8UWNiZR32SEAGAM+8j/BH7RWJqYUJ0xq27x67dOhgW8bKwa7GAcg1/qNGZkr+G/OeeyYRQgRWa7jkwOzU1ycerXIsmw328/CEKVnfum/7y9W3YpEaVdkSX2Dlbhj5NIgIArRWEP5KTGVMrXcnK3fv/7jk4x9O95OQxB5s1GnLp6u7Dx1dwUTQtfvch+E7QiZGDt86fflFsYrr7wGwuau+heV8iPwzqvaZXl0Xhn14/fX6F6Aw7VyuGEcSgdpQVhD9YKSMW66rI3bxz2M+nYrtW422sHYv7VW7SYOCVG3/HJ0RxsWCVdGo7tZCjh0gkDizf5POXdxASG/f5/sMz9Wr1AMvF1qZQyybDTcTmRHeICMsyCbHE6EFZQfiDYQhDdALDMG/ePyhRvJoiBJQF6vCbt0Hcqouzr5lZxjQF5uY28JuUHBcVHQILri5FFFt5eZQmuoQilDRNSowd9K0gPMKybLpOhCU9PU0qlZw8swH+lMPjEzOsFYpS8QRNTJJZDmamX2dFMTW1ILoEmkB2jsbvtEZZQfhDJKZSk3TSFWJqag7qUCmgefmy9ZXDodWTy1ZWlrIRNGmSFEVISmoi0RlJ0fJBKwLoCkNZQfjDzsk0JlJXPazuhUskp8QX86vEraanSyKjQ+ztXHPZxMFeNl/B2/cPuLYPbPLi1U0rK129AxkdlkCL8pog1yhA3wrCHz6lLdNTdeVZaN5oyMMnF2/cOSLzs7wL2rF3ym9/DIPGUS6b2Nu5+HpX+Pfcxk+f30kkqTv/npb3vNjfQVJ0ip2jDvvX9QeUFYQ/qjdzZKRsanw60QFFfAJGD9kOPtqZi5r+tnVEckpCn25LTEzMct+qS/sZ3p5lV67vOWVuPUsL26qBPxKdTWyWlpxWItCGCACcHQ7hlS0z37KsuGh1wU1MHxuaHPIkYujSokQAoLWC8Erdti4piYJ43S4b4S8/u/roclCMPoEuW4RX/CpYmv5Nv7sT4VNJtTP17oN/DxxdrDIKGilJyXEqo6pVat2q6c9ES4Br5vcdY1VGMYwU+qopVS6Y+j/0rF+7l8qtkqLTJKnS9iNy65YyJrARhPCNNIWsn/TSv3ERlbHQHSNR6vHNFiUWq3Z5ikQm0MdMtEdyssaTRIrFpupcOY/PvytV0aZ+F6F88BCtFYRvROakeEWbJxfela7rkzMWhEOddvCJhYXWfKtv7nyysBAJR1MI+laQAqFJT1dbR5OXV0OJsRPxNColLrnPLB8iJLARhBQYJ7Z9fv8kqWQdT2KkfAiOSo5NGDivCBEYaK0gBUazXs7WDvTTi++JMfLqemhSjBA1haC1ghQ4J7dFvHqQYONk6R1gJJ9h/vQq9svbaBtHkx6TvYkgQVlBCp60JLJj4duUJKmFvblHGWdTC0N9G+/Dg8/xX5IowlZt5lSpvuA+vawAZQXRF17cS7xy9EtibDotpsUmIlMrE1MzE9qUzlFEKfkEAxmwhKWhGH8N5RYzoijydYAJJU/BKiXIklFGiCyWVVpls+UmC8gIoUUUk85KktNTE9PS02RzM5iY0aUq29Zp70SEDcoKondcOxb94UVifHR6uoSRprOMNGsRpSiVr+18VQt1YTlURK4QrDyGYpXiKJpis8wOyW2plJssOUWbEBFFi8SUla3Y2cu89o9OplaCeEE5T1BWEATRMjgcDkEQLYOygiCIlkFZQRBEy6CsIAiiZVBWEATRMigrCIJomf8DAAD//8WxgA8AAAAGSURBVAMAMBotgJpqbKYAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d2da908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(name='David Lee', role='AI Product Manager', affiliation='ProductAI Inc.', description=\"Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8d1df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Name: David Lee\\nRole: AI Product Manager\\nAffiliation: ProductAI Inc.\\nDescription: Analyzes Langgraph's impact on AI product development, focusing on its ability to streamline agent orchestration, improve model performance, and enhance user experiences. Concerned with product differentiation, competitive advantage, and market adoption of Langgraph-powered solutions.\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst.persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "420e492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db214534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8fdde6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"So you said you were writing an article on Langchain?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1a2c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "search_query='LangGraph streamlines agent orchestration compared to previous methods'\n",
      "*******************************\n",
      "search_query='LangGraph financial analysis multi-agent system'\n"
     ]
    }
   ],
   "source": [
    "interview = interview_graph.invoke({\"analyst\": analyst, \"messages\": messages, \"max_num_turns\": 2}, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3c46c91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Langgraph: Orchestrating the Future of AI Product Development\n",
       "\n",
       "### Summary\n",
       "\n",
       "LangGraph is emerging as a pivotal framework for orchestrating complex interactions within multi-agent systems, streamlining workflows, and optimizing task allocation [1]. It simplifies the development of state machines by representing them as graphs, making them easier to manage compared to traditional methods like LangChain [1]. This is particularly beneficial for complex Large Language Model (LLM) applications [1]. LangGraph's architecture, built around a directed acyclic graph (DAG) structure, efficiently manages workflows, streamlining complex processes as more agents connect and resource demands grow [2].\n",
       "\n",
       "While LangGraph offers powerful orchestration capabilities, its operational demands, including debugging, monitoring, and infrastructure requirements, can be overwhelming [2]. Platforms like Latenode simplify these complexities by offering managed infrastructure, intuitive workflow mapping, and streamlined error tracing [2]. This allows teams to focus on refining agent logic rather than grappling with the intricacies of distributed systems management [2].\n",
       "\n",
       "LangGraph is trusted by companies shaping the future of agents, including Klarna, Replit, and Elastic [3]. It is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents [3]. Before using LangGraph, it is recommended to familiarize oneself with the components used to build agents, starting with models and tools [3].\n",
       "\n",
       "LangGraph's ability to visualize the graph, isolate failures, and re-run just the failed node saves operational headaches [5]. In a multi-agent architecture design, LangGraph provides powerful workflow orchestration capabilities that align well with the dynamic nature of financial analysis [4]. It supports both directional and recursive workflows, so you can model everything from straightforward sequential processes to complex iterative analyses that evolve based on intermediate findings [4].\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] https://medium.com/@shubham.shardul2019/is-langgraph-the-ultimate-multi-agent-maestro-explore-its-potential-and-hidden-hurdles-c7e454a3e089\n",
       "[2] https://latenode.com/blog/langgraph-multi-agent-orchestration-complete-framework-guide-architecture-analysis-2025\n",
       "[3] https://docs.langchain.com/oss/python/langgraph/overview\n",
       "[4] https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-financial-analysis-agent-with-langgraph-and-strands-agents/\n",
       "[5] https://www.auxiliobits.com/blog/langchain-vs-langgraph-choosing-the-right-orchestration-framework-for-agentic-automation/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70355645",
   "metadata": {},
   "source": [
    "## third Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "82cf6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "import operator\n",
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str # Research topic\n",
    "    max_analysts: int # Number of analysts\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    analysts: List[Analyst] # Analyst asking questions\n",
    "    sections: Annotated[list, operator.add] # Send() API key\n",
    "    introduction: str # Introduction for the final report\n",
    "    content: str # Content for the final report\n",
    "    conclusion: str # Conclusion for the final report\n",
    "    final_report: str # Final report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ae420268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94b8b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_all_interviews(state:ResearchGraphState):\n",
    "    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\" \n",
    "    \n",
    "    #check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # Return to create_analysts\n",
    "        return \"create_analysts\"\n",
    "    \n",
    "    # Otherwise kick off interviews in parallel via Send() API\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [Send(\"conduct_interview\", {\"analyst\": analyst,\n",
    "                                        \"messages\": [HumanMessage(\n",
    "                                            content=f\"So you said you were writing an article on {topic}?\"\n",
    "                                        )\n",
    "                                                ]}) for analyst in state[\"analysts\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e497a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_report(state:ResearchGraphState):\n",
    "#     \"\"\"_summary_\n",
    "\n",
    "#     Args:\n",
    "#         state (ResearchGraphState): _description_\n",
    "#     \"\"\"\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1afb3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_introduction(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")]) \n",
    "    return {\"introduction\": intro.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b28a3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report(state:ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")]) \n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78f30ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conclusion(stae:ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")]) \n",
    "    return {\"conclusion\": conclusion.content}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "73aa6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n",
    "    # Save full final report\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36abd58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x127306d50>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add nodes and edges \n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analyst)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\",write_report)\n",
    "builder.add_node(\"write_introduction\",write_introduction)\n",
    "builder.add_node(\"write_conclusion\",write_conclusion)\n",
    "builder.add_node(\"finalize_report\",finalize_report)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46ed45",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c888e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebea3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
